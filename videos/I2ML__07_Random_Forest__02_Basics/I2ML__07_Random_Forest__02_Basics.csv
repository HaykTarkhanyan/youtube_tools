index,time_range,text
1,"00:00:00,160 --> 00:00:05,480",hi Welcome to our next session here in
2,"00:00:02,879 --> 00:00:07,439",our yeah chapter on the random Forest
3,"00:00:05,480 --> 00:00:10,599",algorithm where in this session here I
4,"00:00:07,439 --> 00:00:12,920",will now truly introduce and discuss the
5,"00:00:10,599 --> 00:00:16,080",fundamentals of the random Forest
6,"00:00:12,920 --> 00:00:18,600",algorithm okay we'll understand how this
7,"00:00:16,080 --> 00:00:20,800",extends the idea of bagging we'll then
8,"00:00:18,600 --> 00:00:23,359",spend some time to understand the
9,"00:00:20,800 --> 00:00:26,560",general idea behind the decorrelation of
10,"00:00:23,359 --> 00:00:29,679",trees so the feature subsampling in the
11,"00:00:26,560 --> 00:00:31,519",ren Forest algorithm we'll discuss the
12,"00:00:29,679 --> 00:00:34,320",effect
13,"00:00:31,519 --> 00:00:36,320",of yeah um the I would say the most
14,"00:00:34,320 --> 00:00:39,200",important hyper parames of the random
15,"00:00:36,320 --> 00:00:41,079",Forest algorithm and then discuss
16,"00:00:39,200 --> 00:00:43,000",whether random Forest can actually
17,"00:00:41,079 --> 00:00:44,760",overfit answer little bit of
18,"00:00:43,000 --> 00:00:47,600",foreshadowing yes they
19,"00:00:44,760 --> 00:00:50,120",can Okay so let's start we have
20,"00:00:47,600 --> 00:00:51,760",discussed the card algorithm before so
21,"00:00:50,120 --> 00:00:54,239",classification and regression trees by
22,"00:00:51,760 --> 00:00:56,079",bran that actually has quite a few
23,"00:00:54,239 --> 00:00:58,120",appealing features it's quite
24,"00:00:56,079 --> 00:01:00,320",interoperable so it's easy to understand
25,"00:00:58,120 --> 00:01:03,359",it's easy to explain because it's a
26,"00:01:00,320 --> 00:01:06,040",simple decision tree it is invariant
27,"00:01:03,359 --> 00:01:09,240",under rank preserving Transformations so
28,"00:01:06,040 --> 00:01:11,759",as long as we keep yeah the order of
29,"00:01:09,240 --> 00:01:15,000",feature values fixed while we transform
30,"00:01:11,759 --> 00:01:17,920",them so by shifting them scaling them or
31,"00:01:15,000 --> 00:01:20,479",doing any other type of monotonous
32,"00:01:17,920 --> 00:01:23,200",transformation the the resulting model
33,"00:01:20,479 --> 00:01:24,960",the resulting decision tree will not be
34,"00:01:23,200 --> 00:01:27,079",substantially influenced by this the
35,"00:01:24,960 --> 00:01:28,439",location of the split points will vary
36,"00:01:27,079 --> 00:01:30,640",but we're basically fitting the same
37,"00:01:28,439 --> 00:01:32,960",type the same type of model model and
38,"00:01:30,640 --> 00:01:35,159",particularly the a model with the same
39,"00:01:32,960 --> 00:01:38,280",predictive power and
40,"00:01:35,159 --> 00:01:40,399",performance trees can work with cacor
41,"00:01:38,280 --> 00:01:42,240",categorical features as they can with
42,"00:01:40,399 --> 00:01:45,360",numerical features and they even can
43,"00:01:42,240 --> 00:01:49,280",work with data where there are missing
44,"00:01:45,360 --> 00:01:52,880",values present in the features Yeah by
45,"00:01:49,280 --> 00:01:54,680",using so-called surrogate splits now
46,"00:01:52,880 --> 00:01:57,600",here's a quote from hasted tipani and
47,"00:01:54,680 --> 00:02:00,399",Freedman which says trees have exactly
48,"00:01:57,600 --> 00:02:02,320",only one aspect that prevents them from
49,"00:02:00,399 --> 00:02:05,039",being an Ideal tool for predictive
50,"00:02:02,320 --> 00:02:06,920",learning not sure whether we go that far
51,"00:02:05,039 --> 00:02:09,160",but they say h they could be a bit more
52,"00:02:06,920 --> 00:02:11,520",accurate yeah they're not the most
53,"00:02:09,160 --> 00:02:14,040",powerful predictor the idea of the
54,"00:02:11,520 --> 00:02:16,840",random Forest is to basically attack
55,"00:02:14,040 --> 00:02:19,879",exactly that problem by creating a
56,"00:02:16,840 --> 00:02:21,760",begging Ensemble on top of decision
57,"00:02:19,879 --> 00:02:23,560",trees we'll see that I guess this will
58,"00:02:21,760 --> 00:02:28,720",become somewhat obvious that we'll lose
59,"00:02:23,560 --> 00:02:30,640",the inability aspect of the single tree
60,"00:02:28,720 --> 00:02:33,599",but all of these other
61,"00:02:30,640 --> 00:02:36,080",advantages of the card algorithm that
62,"00:02:33,599 --> 00:02:39,000",inherited into the random forest and we
63,"00:02:36,080 --> 00:02:41,760",will also create a usually a way
64,"00:02:39,000 --> 00:02:44,760",stronger predictor by embling in
65,"00:02:41,760 --> 00:02:48,120",comparison to using a single decision
66,"00:02:44,760 --> 00:02:50,920",tree so how does this work now we create
67,"00:02:48,120 --> 00:02:52,720",a begging Ensemble out of a bunch of
68,"00:02:50,920 --> 00:02:54,239",decision trees we have discussed how
69,"00:02:52,720 --> 00:02:56,440",that works right so I don't have to
70,"00:02:54,239 --> 00:02:59,040",repeat this year again bman when he
71,"00:02:56,440 --> 00:03:01,480",introduced that which I think was around
72,"00:02:59,040 --> 00:03:04,000",1998 eight he thought about this a
73,"00:03:01,480 --> 00:03:05,799",little bit more more deeply and he
74,"00:03:04,000 --> 00:03:08,560",wanted to figure out whether he can
75,"00:03:05,799 --> 00:03:11,560",could even improve the predictive
76,"00:03:08,560 --> 00:03:13,959",performance of such a begging Ensemble
77,"00:03:11,560 --> 00:03:17,120",further and then he figured out that the
78,"00:03:13,959 --> 00:03:19,480",correlation among the emble members
79,"00:03:17,120 --> 00:03:21,400",might actually be a problem or that to
80,"00:03:19,480 --> 00:03:23,840",State it more positively That by
81,"00:03:21,400 --> 00:03:25,599",decorrelating The Ensemble members he
82,"00:03:23,840 --> 00:03:28,519",could he could potentially improve
83,"00:03:25,599 --> 00:03:30,959",productive performance and we'll discuss
84,"00:03:28,519 --> 00:03:34,200",exactly that now here's some intuition
85,"00:03:30,959 --> 00:03:37,959",behind that how does bagging how does
86,"00:03:34,200 --> 00:03:41,040",bagging work we have our model fad and
87,"00:03:37,959 --> 00:03:44,599",our model f hat is going to be a simple
88,"00:03:41,040 --> 00:03:46,599",average across capital M base Learners
89,"00:03:44,599 --> 00:03:49,879",we already know we at least have a
90,"00:03:46,599 --> 00:03:54,000",little bit of intuition that in yeah in
91,"00:03:49,879 --> 00:03:56,560",terms of prediction error the variance
92,"00:03:54,000 --> 00:03:58,040",of such a model and that the variance
93,"00:03:56,560 --> 00:04:00,400",here of our assemble because that's what
94,"00:03:58,040 --> 00:04:03,040",we care about is made
95,"00:04:00,400 --> 00:04:05,480",yeah yeah not the best thing so if the
96,"00:04:03,040 --> 00:04:08,120",model is very unstable if it's very if
97,"00:04:05,480 --> 00:04:11,720",it has a has a large variance that's
98,"00:04:08,120 --> 00:04:13,720",usually also a that's a component in our
99,"00:04:11,720 --> 00:04:15,919",generalization error in this case we
100,"00:04:13,720 --> 00:04:18,160",actually like the variance of the
101,"00:04:15,919 --> 00:04:20,079",underlying base learning algorithm
102,"00:04:18,160 --> 00:04:22,800",because that's something we can now
103,"00:04:20,079 --> 00:04:27,320",smooth out over and that we can attack
104,"00:04:22,800 --> 00:04:29,720",and reduce but the variance of our
105,"00:04:27,320 --> 00:04:31,560",resulting Ensemble that should be
106,"00:04:29,720 --> 00:04:33,199",reasonably small if that's too high
107,"00:04:31,560 --> 00:04:36,240",that's usually large component also in
108,"00:04:33,199 --> 00:04:38,680",our generalization error now do we also
109,"00:04:36,240 --> 00:04:41,919",have an intuition how variance goes down
110,"00:04:38,680 --> 00:04:44,440",if we yeah look at some some fundamental
111,"00:04:41,919 --> 00:04:47,320",theorems of probability Theory or
112,"00:04:44,440 --> 00:04:50,600",statistics we can expect that variance
113,"00:04:47,320 --> 00:04:52,960",goes down linearly if we average out
114,"00:04:50,600 --> 00:04:54,919",over capital M random
115,"00:04:52,960 --> 00:04:56,840",variables if they are
116,"00:04:54,919 --> 00:04:58,800",independent okay if things are
117,"00:04:56,840 --> 00:05:01,560",independent let's say they are IID
118,"00:04:58,800 --> 00:05:04,479",distributed we compute their average and
119,"00:05:01,560 --> 00:05:07,840",then we look at the variance of the
120,"00:05:04,479 --> 00:05:10,400",average in comparison to the average of
121,"00:05:07,840 --> 00:05:12,880",one single random variable variance goes
122,"00:05:10,400 --> 00:05:12,880",down
123,"00:05:13,039 --> 00:05:21,240",linearly now can we
124,"00:05:16,160 --> 00:05:23,680",expect our Ensemble members here in our
125,"00:05:21,240 --> 00:05:26,039",begging Ensemble to be independent
126,"00:05:23,680 --> 00:05:28,800",likely wrong right can't really expect
127,"00:05:26,039 --> 00:05:31,240",that because our individual Ensemble
128,"00:05:28,800 --> 00:05:33,880",members me numbers will be correlated
129,"00:05:31,240 --> 00:05:36,639",and this is because our bootstrap
130,"00:05:33,880 --> 00:05:39,840",samples are um fairly similar right we
131,"00:05:36,639 --> 00:05:42,199",are sampling from the same underlying um
132,"00:05:39,840 --> 00:05:45,840",training data set the bootstrap datas
133,"00:05:42,199 --> 00:05:48,720",are um yeah fairly similar and this will
134,"00:05:45,840 --> 00:05:50,680",also make our resulting models somewhat
135,"00:05:48,720 --> 00:05:54,560",similar and
136,"00:05:50,680 --> 00:05:58,840",correlated now if that's the case how
137,"00:05:54,560 --> 00:06:01,840",about then we do um yeah more uh deeper
138,"00:05:58,840 --> 00:06:04,880",analysis of what the variance of our
139,"00:06:01,840 --> 00:06:07,160",resulting model actually is okay let's
140,"00:06:04,880 --> 00:06:09,680",let's do exactly that so just assume two
141,"00:06:07,160 --> 00:06:12,440",things so this analysis here is a little
142,"00:06:09,680 --> 00:06:15,199",bit simplified but I guess it's going to
143,"00:06:12,440 --> 00:06:16,440",be instructive anyway so I'll assume
144,"00:06:15,199 --> 00:06:18,319",that yeah and this is where I'm
145,"00:06:16,440 --> 00:06:20,639",simplifying things a little bit so this
146,"00:06:18,319 --> 00:06:23,000",here is of course a function these these
147,"00:06:20,639 --> 00:06:24,759",base Learners kind of treat them here as
148,"00:06:23,000 --> 00:06:27,880",a scalar random
149,"00:06:24,759 --> 00:06:32,319",variable and I'm assuming they have all
150,"00:06:27,880 --> 00:06:35,560",the same variance Sigma squ and I'm also
151,"00:06:32,319 --> 00:06:38,039",assuming they have a fixed correlation
152,"00:06:35,560 --> 00:06:41,479",between any yeah any two of them and
153,"00:06:38,039 --> 00:06:43,919",that correlation all denote by row what
154,"00:06:41,479 --> 00:06:46,400",is now the variance of our Ensemble fad
155,"00:06:43,919 --> 00:06:48,639",we just insert our definition of the
156,"00:06:46,400 --> 00:06:53,120",bagging Ensemble that's just an average
157,"00:06:48,639 --> 00:06:55,240",yeah over these base Learners bad n now
158,"00:06:53,120 --> 00:06:58,440",there is now a nice formula for what
159,"00:06:55,240 --> 00:07:00,080",happens with the variance even if stuff
160,"00:06:58,440 --> 00:07:01,680",is not completely independ depent if it
161,"00:07:00,080 --> 00:07:04,680",would be independent I could now just
162,"00:07:01,680 --> 00:07:06,800",draw the variance operator kind of in a
163,"00:07:04,680 --> 00:07:08,440",in a linear fashion inside of the sum
164,"00:07:06,800 --> 00:07:10,759",but here I can't do that because stuff
165,"00:07:08,440 --> 00:07:13,199",is correlated but I can use this more
166,"00:07:10,759 --> 00:07:14,280",general formula here we have this which
167,"00:07:13,199 --> 00:07:17,080",in a certain sense looks a little bit
168,"00:07:14,280 --> 00:07:19,360",like a binomial formula so these squar
169,"00:07:17,080 --> 00:07:23,000",terms here but here I have these
170,"00:07:19,360 --> 00:07:25,400",covariance terms between Pairs of random
171,"00:07:23,000 --> 00:07:26,879",variables okay can write it like that
172,"00:07:25,400 --> 00:07:29,639",and now the only thing I will do is I
173,"00:07:26,879 --> 00:07:33,160",will insert my my constants my
174,"00:07:29,639 --> 00:07:35,360",abbreviations so the variance of bad M
175,"00:07:33,160 --> 00:07:37,800",so that's supposed to be Sigma squ so
176,"00:07:35,360 --> 00:07:40,520",we'll insert that here and now there's a
177,"00:07:37,800 --> 00:07:44,720",sum over M of these things so it's
178,"00:07:40,520 --> 00:07:47,120",capital M of Sigma squ I can also insert
179,"00:07:44,720 --> 00:07:48,680",my correlation here now I have to be a
180,"00:07:47,120 --> 00:07:51,440",little bit careful because correlation
181,"00:07:48,680 --> 00:07:54,039",is not the same as Co variance but
182,"00:07:51,440 --> 00:07:56,639",correlation multiplied by the standard
183,"00:07:54,039 --> 00:07:58,000",deviation of of the one variable
184,"00:07:56,639 --> 00:08:00,159",multipli by the standard deviation of
185,"00:07:58,000 --> 00:08:01,639",the other variable that's the same as Co
186,"00:08:00,159 --> 00:08:03,919",variance and because they all have the
187,"00:08:01,639 --> 00:08:08,759",same variance so it's co variance
188,"00:08:03,919 --> 00:08:12,440",between I and J is or sorry M and J is
189,"00:08:08,759 --> 00:08:14,879",simply row * Sigma * Sigma so * Sigma
190,"00:08:12,440 --> 00:08:18,960",squ now all of these terms here in this
191,"00:08:14,879 --> 00:08:23,319",sum they are now all also all the same
192,"00:08:18,960 --> 00:08:29,199",it's a sum over let me check capital M
193,"00:08:23,319 --> 00:08:32,959",over 2 PA so this is M * m - 1 / 2 we
194,"00:08:29,199 --> 00:08:35,440",cop over the two oh and I forgot about
195,"00:08:32,959 --> 00:08:37,479",this guy here so the one divided by m
196,"00:08:35,440 --> 00:08:39,839",that's inside of the variant so we draw
197,"00:08:37,479 --> 00:08:43,800",that to the outside I'm actually already
198,"00:08:39,839 --> 00:08:46,080",doing that here this becomes 1 / m^2 and
199,"00:08:43,800 --> 00:08:49,160",now you just simplify things a little
200,"00:08:46,080 --> 00:08:51,519",bit you collect terms a bit I'll remove
201,"00:08:49,160 --> 00:08:53,480",all of my annoying scribbling here and
202,"00:08:51,519 --> 00:08:55,519",then we look at the resulting formula
203,"00:08:53,480 --> 00:08:57,880",and that has actually pretty nice
204,"00:08:55,519 --> 00:09:02,680",structure it's quite instructive so we
205,"00:08:57,880 --> 00:09:05,600",can see here the sigma squ ided by m so
206,"00:09:02,680 --> 00:09:07,800",that would be the optimal reduction in
207,"00:09:05,600 --> 00:09:11,040",variance we could ever hope for okay if
208,"00:09:07,800 --> 00:09:14,200",there would be no correlation and here
209,"00:09:11,040 --> 00:09:15,880",the sigma squar that's the worst that
210,"00:09:14,200 --> 00:09:19,000",can happen to
211,"00:09:15,880 --> 00:09:21,120",us because that would be no reduction
212,"00:09:19,000 --> 00:09:24,519",variance you could for example think of
213,"00:09:21,120 --> 00:09:26,040",all models being exact copies of each
214,"00:09:24,519 --> 00:09:28,040",other yeah so really deterministic
215,"00:09:26,040 --> 00:09:29,519",copies then if you average those there
216,"00:09:28,040 --> 00:09:31,760",will be no reduction variance right
217,"00:09:29,519 --> 00:09:35,320",because you're just aing out averaging
218,"00:09:31,760 --> 00:09:38,440",out over all of the same things now this
219,"00:09:35,320 --> 00:09:42,240",formula here tells us that between this
220,"00:09:38,440 --> 00:09:45,839",zero reduction in variance and between
221,"00:09:42,240 --> 00:09:48,320",this optimal linear reduction in
222,"00:09:45,839 --> 00:09:49,920",invariance what we get is a convex
223,"00:09:48,320 --> 00:09:51,959",combination of the two so you can see
224,"00:09:49,920 --> 00:09:55,240",the convex combination here by one minus
225,"00:09:51,959 --> 00:09:57,680",row here and having the RO there and
226,"00:09:55,240 --> 00:10:00,600",this convex combination is controlled by
227,"00:09:57,680 --> 00:10:02,959",the correlation between test of models
228,"00:10:00,600 --> 00:10:06,079",of random variables the stronger the
229,"00:10:02,959 --> 00:10:08,839",correlation the less reduction you get
230,"00:10:06,079 --> 00:10:12,160",the closer you are to this yeah to this
231,"00:10:08,839 --> 00:10:16,360",zero reduction in variance and if so if
232,"00:10:12,160 --> 00:10:19,000",for example yeah if if robot be one yeah
233,"00:10:16,360 --> 00:10:22,040",then this here becomes zero and we just
234,"00:10:19,000 --> 00:10:25,279",end up with this very bad scenario where
235,"00:10:22,040 --> 00:10:27,959",there's no reduction and if R would be
236,"00:10:25,279 --> 00:10:30,279",zero there would be no correlation yeah
237,"00:10:27,959 --> 00:10:31,600",then we have in this scenario of
238,"00:10:30,279 --> 00:10:33,000",shouldn't say independence because we
239,"00:10:31,600 --> 00:10:34,680",don't know that whether things are
240,"00:10:33,000 --> 00:10:36,639",normally distributed here but in this
241,"00:10:34,680 --> 00:10:37,839",uncorrelated scenario where we get
242,"00:10:36,639 --> 00:10:40,399",linear
243,"00:10:37,839 --> 00:10:42,800",reduction this is somewhat simplified
244,"00:10:40,399 --> 00:10:45,279",I'll add a caveat on the next slide but
245,"00:10:42,800 --> 00:10:48,480",in general now the idea is can we maybe
246,"00:10:45,279 --> 00:10:51,240",decorrelate trees bit more to reduce
247,"00:10:48,480 --> 00:10:53,920",onal variance and that will maybe also
248,"00:10:51,240 --> 00:10:57,440",help us to reduce production error I
249,"00:10:53,920 --> 00:10:59,800",guess that is a valid path forward now
250,"00:10:57,440 --> 00:11:02,839",and this is exactly what bman did so he
251,"00:10:59,800 --> 00:11:04,800",now thought about a simple randomization
252,"00:11:02,839 --> 00:11:07,399",procedure that he could bring into the
253,"00:11:04,800 --> 00:11:09,240",tree fitting algorithm so what we are
254,"00:11:07,399 --> 00:11:12,760",now doing is when we are fitting our
255,"00:11:09,240 --> 00:11:14,639",tree we greedily construct more and more
256,"00:11:12,760 --> 00:11:17,639",nodes in the tree we construct more and
257,"00:11:14,639 --> 00:11:19,720",more splits and in each individual split
258,"00:11:17,639 --> 00:11:21,440",and when we construct that we search for
259,"00:11:19,720 --> 00:11:23,680",the best split and when we search for
260,"00:11:21,440 --> 00:11:26,120",the best split we search over all
261,"00:11:23,680 --> 00:11:28,839",potential features and then over all
262,"00:11:26,120 --> 00:11:31,360",possible split points to find the one
263,"00:11:28,839 --> 00:11:34,240",which would reduces our risk the most so
264,"00:11:31,360 --> 00:11:38,399",this randomization procedure now says do
265,"00:11:34,240 --> 00:11:41,200",not search over all features but only
266,"00:11:38,399 --> 00:11:43,440",consider a certain subset and that
267,"00:11:41,200 --> 00:11:45,120",random subset of the I don't know
268,"00:11:43,440 --> 00:11:47,959",feasible features you're allowed to look
269,"00:11:45,120 --> 00:11:50,920",at in Split finding that's randomly
270,"00:11:47,959 --> 00:11:53,279",sampled again and again um per note you
271,"00:11:50,920 --> 00:11:55,880",could do this also per tree but usually
272,"00:11:53,279 --> 00:11:58,200",this is done on a per note basis if you
273,"00:11:55,880 --> 00:12:00,480",look in some at some implementations
274,"00:11:58,200 --> 00:12:03,360",especially in r this parameter is called
275,"00:12:00,480 --> 00:12:05,720",M try I guess that might be for maximum
276,"00:12:03,360 --> 00:12:08,320",number of features to try out for
277,"00:12:05,720 --> 00:12:10,200",finding a split I'm not exactly sure
278,"00:12:08,320 --> 00:12:12,519",whether that's the the interpretation of
279,"00:12:10,200 --> 00:12:14,360",the m and we will only consider these
280,"00:12:12,519 --> 00:12:16,600",features now for finding the best split
281,"00:12:14,360 --> 00:12:19,839",so maybe here this is our complete let
282,"00:12:16,600 --> 00:12:21,360",me actually switch my pen tool on again
283,"00:12:19,839 --> 00:12:23,480",so that maybe that's our complete data
284,"00:12:21,360 --> 00:12:24,920",set here so with our Target column here
285,"00:12:23,480 --> 00:12:27,040",we might have a bunch of different
286,"00:12:24,920 --> 00:12:29,399",features maybe there's four but now we
287,"00:12:27,040 --> 00:12:31,600",will only allow color and length yeah
288,"00:12:29,399 --> 00:12:34,360",for this I don't know um training data
289,"00:12:31,600 --> 00:12:36,519",set of measurements of bananas or
290,"00:12:34,360 --> 00:12:39,199",objects which can be bananas and maybe
291,"00:12:36,519 --> 00:12:42,920",also something else and we will not only
292,"00:12:39,199 --> 00:12:44,680",search over them okay um and the idea is
293,"00:12:42,920 --> 00:12:47,519",I guess that's some pretty obvious so
294,"00:12:44,680 --> 00:12:50,000",the more we decorrelate or we the
295,"00:12:47,519 --> 00:12:52,720",smaller we set this mry here the more
296,"00:12:50,000 --> 00:12:55,440",trees become decorrelated but also the
297,"00:12:52,720 --> 00:12:58,120",more rendom our trees become in the
298,"00:12:55,440 --> 00:13:00,279",extreme case for example for a specific
299,"00:12:58,120 --> 00:13:03,279",node and you you might have a large
300,"00:13:00,279 --> 00:13:05,760",number of features maybe 100 and M2
301,"00:13:03,279 --> 00:13:07,639",would be set to one so you now pick one
302,"00:13:05,760 --> 00:13:09,480",random feature and then you only allowed
303,"00:13:07,639 --> 00:13:11,199",to search for the best split point for
304,"00:13:09,480 --> 00:13:12,480",that feature which might be a very bad
305,"00:13:11,199 --> 00:13:14,120",one yeah it might not carry any
306,"00:13:12,480 --> 00:13:15,959",information it might not even be
307,"00:13:14,120 --> 00:13:18,240",associated with wi might be some
308,"00:13:15,959 --> 00:13:20,560",something like a noise feature but so
309,"00:13:18,240 --> 00:13:22,920",you but you're only allowed to use that
310,"00:13:20,560 --> 00:13:25,360",to find a split at that note guess for a
311,"00:13:22,920 --> 00:13:27,440",newe going to sample again but this
312,"00:13:25,360 --> 00:13:29,120",makes the trees fairly random and this
313,"00:13:27,440 --> 00:13:31,240",will also have a negative effect if you
314,"00:13:29,120 --> 00:13:33,040",overdo that but hopefully there might be
315,"00:13:31,240 --> 00:13:35,959",something like a sweet sweet spot in the
316,"00:13:33,040 --> 00:13:38,600",Middle where this decorrelation helps to
317,"00:13:35,959 --> 00:13:40,680",reduce variance as we discussed before
318,"00:13:38,600 --> 00:13:43,160",but the trees are not becoming super
319,"00:13:40,680 --> 00:13:46,320",random and this is usually exactly what
320,"00:13:43,160 --> 00:13:49,720",happens I've created here to simp yeah
321,"00:13:46,320 --> 00:13:51,560",visualizations to simple examples so I
322,"00:13:49,720 --> 00:13:53,759",used the classification data set the
323,"00:13:51,560 --> 00:13:55,480",spam data set I use this empty cars data
324,"00:13:53,759 --> 00:13:59,480",set for regression which we've also seen
325,"00:13:55,480 --> 00:14:01,880",before and then now varying mry as
326,"00:13:59,480 --> 00:14:07,680",percentage of total features in my data
327,"00:14:01,880 --> 00:14:10,240",set from 0 to 100 and I'm now plotting
328,"00:14:07,680 --> 00:14:12,000",the cross validated error of these
329,"00:14:10,240 --> 00:14:15,360",different mry settings and what you can
330,"00:14:12,000 --> 00:14:18,959",see is that if we set mry to very low
331,"00:14:15,360 --> 00:14:21,800",value to so maybe to one then this
332,"00:14:18,959 --> 00:14:24,600",doesn't work too well at least in in
333,"00:14:21,800 --> 00:14:26,759",this data set here so our trees are
334,"00:14:24,600 --> 00:14:29,320",becoming way too random yeah we are not
335,"00:14:26,759 --> 00:14:30,680",really finding uh good splits anymore in
336,"00:14:29,320 --> 00:14:34,680",our trees and then the variance
337,"00:14:30,680 --> 00:14:38,360",reduction also doesn't save us and then
338,"00:14:34,680 --> 00:14:41,680",M becomes a bit larger and that works
339,"00:14:38,360 --> 00:14:44,759",actually fairly well and at some point m
340,"00:14:41,680 --> 00:14:48,000",is too large yeah and this would here
341,"00:14:44,759 --> 00:14:50,000",for m equals 100% that would be the
342,"00:14:48,000 --> 00:14:52,560",original begging algorithm but you can
343,"00:14:50,000 --> 00:14:55,800",see that usually there's a spot where we
344,"00:14:52,560 --> 00:15:00,079",perform actually considerably better
345,"00:14:55,800 --> 00:15:02,480",than by just fixing mry to 100% And and
346,"00:15:00,079 --> 00:15:04,600",basically running the original bagging
347,"00:15:02,480 --> 00:15:07,279",algorithm so there is some Merit there
348,"00:15:04,600 --> 00:15:11,040",is some benefit in running this this
349,"00:15:07,279 --> 00:15:14,320",decorrelated version of The
350,"00:15:11,040 --> 00:15:16,759",Ensemble quite good default values exist
351,"00:15:14,320 --> 00:15:19,399",which have being empirically I would say
352,"00:15:16,759 --> 00:15:22,199",nicely validated so for classification
353,"00:15:19,399 --> 00:15:24,440",very often we set this m try here to
354,"00:15:22,199 --> 00:15:27,399",square root of number of features so
355,"00:15:24,440 --> 00:15:29,839",that's also due to Bron and for
356,"00:15:27,399 --> 00:15:32,440",regression we usually set it to somewhat
357,"00:15:29,839 --> 00:15:36,319",larger value so P ided by 3 usually
358,"00:15:32,440 --> 00:15:38,160",works quite well I've even yeah looked
359,"00:15:36,319 --> 00:15:40,319",at what would happen if I would have
360,"00:15:38,160 --> 00:15:43,199",used these default settings here for my
361,"00:15:40,319 --> 00:15:46,079",two examples turns out yeah I was quite
362,"00:15:43,199 --> 00:15:47,959",lucky they do work quite well here
363,"00:15:46,079 --> 00:15:50,319",especially for the classification case
364,"00:15:47,959 --> 00:15:52,040",nearly perfect for regression I guess I
365,"00:15:50,319 --> 00:15:55,040",could have used an even larger setting
366,"00:15:52,040 --> 00:15:58,440",yeah the random Forest algorithm itself
367,"00:15:55,040 --> 00:16:00,079",is not I would say super sensitive to
368,"00:15:58,440 --> 00:16:02,160",its hyper parer settings and these
369,"00:16:00,079 --> 00:16:03,720",defaults usually work fairly well so if
370,"00:16:02,160 --> 00:16:06,639",you just stick to them if you don't do
371,"00:16:03,720 --> 00:16:09,480",any tuning you usually I would say you
372,"00:16:06,639 --> 00:16:11,759",are not completely wrong in being lazy
373,"00:16:09,480 --> 00:16:13,839",here and not tuning your model but if
374,"00:16:11,759 --> 00:16:15,199",you want to tune a parameter this um
375,"00:16:13,839 --> 00:16:17,600",yeah this this feature sampling
376,"00:16:15,199 --> 00:16:19,920",parameter this mry parameter is usually
377,"00:16:17,600 --> 00:16:23,360",the most important one so you can tune
378,"00:16:19,920 --> 00:16:27,519",that um but you usually get I would say
379,"00:16:23,360 --> 00:16:29,279",less less improvements um than for for
380,"00:16:27,519 --> 00:16:30,759",hyper parames of other Machining models
381,"00:16:29,279 --> 00:16:32,839",but you can try it sometimes it does
382,"00:16:30,759 --> 00:16:35,560",work and it's
383,"00:16:32,839 --> 00:16:38,360",worthwhile um we can also control the
384,"00:16:35,560 --> 00:16:40,480",depth of our trees or at the very least
385,"00:16:38,360 --> 00:16:42,600",we can discuss whether we would use
386,"00:16:40,480 --> 00:16:44,600",somewhat shallow or deeper trees here in
387,"00:16:42,600 --> 00:16:47,399",random Forest so in random Forest we
388,"00:16:44,600 --> 00:16:49,560",usually use fully expanded trees so we
389,"00:16:47,399 --> 00:16:52,079",don't really do any aggressive early
390,"00:16:49,560 --> 00:16:53,839",stopping of our tree expansion I think
391,"00:16:52,079 --> 00:16:56,040",we also talked here in the lecture about
392,"00:16:53,839 --> 00:16:57,959",po talk pruning of trees usually all of
393,"00:16:56,040 --> 00:16:59,839",that is not really done so the tree is
394,"00:16:57,959 --> 00:17:02,480",configured in a manner that it's either
395,"00:16:59,839 --> 00:17:05,880",fully or let's say nearly fully expanded
396,"00:17:02,480 --> 00:17:08,640",and that is also done in order to
397,"00:17:05,880 --> 00:17:10,959",increase the variability um the variance
398,"00:17:08,640 --> 00:17:13,520",of each individual tree to get more from
399,"00:17:10,959 --> 00:17:15,280",this you from this smoothing in the
400,"00:17:13,520 --> 00:17:17,640",begging
401,"00:17:15,280 --> 00:17:20,480",procedure we do can control this a
402,"00:17:17,640 --> 00:17:21,959",little bit or can discuss the effects a
403,"00:17:20,480 --> 00:17:23,839",bit so for example there would be a
404,"00:17:21,959 --> 00:17:25,839",parameter which controls the minimum
405,"00:17:23,839 --> 00:17:28,160",number of observations in each decision
406,"00:17:25,839 --> 00:17:31,200",tree node so that's usually called yeah
407,"00:17:28,160 --> 00:17:33,640",Min node size and that if you are below
408,"00:17:31,200 --> 00:17:36,520",below that Min node size you would not
409,"00:17:33,640 --> 00:17:38,960",attempt a further split in your tree by
410,"00:17:36,520 --> 00:17:41,919",setting that to a larger value you can
411,"00:17:38,960 --> 00:17:44,559",actually restrict and constrain
412,"00:17:41,919 --> 00:17:47,440",regularize the size of your trees in
413,"00:17:44,559 --> 00:17:51,000",your emble the default for that is very
414,"00:17:47,440 --> 00:17:52,600",often a low value sometimes around five
415,"00:17:51,000 --> 00:17:53,960",can also vary a little bit between
416,"00:17:52,600 --> 00:17:55,640",regression and classification usually
417,"00:17:53,960 --> 00:17:58,600",for regression you set it to an even
418,"00:17:55,640 --> 00:18:00,640",lower value I have again looked at what
419,"00:17:58,600 --> 00:18:02,799",happen happens on the spam data set for
420,"00:18:00,640 --> 00:18:04,799",classification and on the Mt cars data
421,"00:18:02,799 --> 00:18:06,320",set for regression so especially for
422,"00:18:04,799 --> 00:18:08,760",regression you can see a very clear
423,"00:18:06,320 --> 00:18:10,480",effect here that now the smaller you set
424,"00:18:08,760 --> 00:18:13,480",Min node size you could even set this to
425,"00:18:10,480 --> 00:18:15,280",one the better this works this exactly
426,"00:18:13,480 --> 00:18:17,880",like what I said for classification you
427,"00:18:15,280 --> 00:18:19,640",set this to an even lower value and for
428,"00:18:17,880 --> 00:18:22,240",even here on the spam data set it works
429,"00:18:19,640 --> 00:18:24,240",better if you choose smaller values but
430,"00:18:22,240 --> 00:18:26,720",sometimes can be okay to to constrain
431,"00:18:24,240 --> 00:18:29,760",this a little bit there are also other
432,"00:18:26,720 --> 00:18:32,039",parameters to restrict the depth for
433,"00:18:29,760 --> 00:18:33,960",example the max depth parameter so you
434,"00:18:32,039 --> 00:18:37,039",can just prescribe a value so I don't
435,"00:18:33,960 --> 00:18:39,799",want trees which have more than 100
436,"00:18:37,039 --> 00:18:42,320",terminal nodes or which don't grow over
437,"00:18:39,799 --> 00:18:44,760",more than eight levels or whatever
438,"00:18:42,320 --> 00:18:46,440",usually we don't use that too often so
439,"00:18:44,760 --> 00:18:48,919",in this Ranger package here which is a
440,"00:18:46,440 --> 00:18:50,760",very efficient uh implementation of the
441,"00:18:48,919 --> 00:18:52,280",random forest in C++ and which is
442,"00:18:50,760 --> 00:18:53,960",connected to R in Python so that's
443,"00:18:52,280 --> 00:18:55,799",usually not restricted at all and there
444,"00:18:53,960 --> 00:18:57,520",are also alternative H perameters to
445,"00:18:55,799 --> 00:19:00,679",control the depth of the tree you have
446,"00:18:57,520 --> 00:19:03,919",seen them before and we studied decision
447,"00:19:00,679 --> 00:19:06,440",trees specifically like minimal risk
448,"00:19:03,919 --> 00:19:08,840",reduction to actually require from a
449,"00:19:06,440 --> 00:19:13,679",split from the optimal split to really
450,"00:19:08,840 --> 00:19:13,679",perform this size of termal nodes and so
451,"00:19:14,000 --> 00:19:18,919",on here's a further important kind of
452,"00:19:16,520 --> 00:19:21,039",special hyper parameter onor size so I
453,"00:19:18,919 --> 00:19:23,440",discussed this already a little bit so
454,"00:19:21,039 --> 00:19:25,440",usually random Forest perform better if
455,"00:19:23,440 --> 00:19:27,520",the ooral size is quite large so there
456,"00:19:25,440 --> 00:19:30,120",are sometimes people that for
457,"00:19:27,520 --> 00:19:32,400",computational reasons mainly construct
458,"00:19:30,120 --> 00:19:33,960",smaller ensembles I guess very often
459,"00:19:32,400 --> 00:19:36,480",also if you do bagging on top of new
460,"00:19:33,960 --> 00:19:39,280",networks but sometimes also people run I
461,"00:19:36,480 --> 00:19:41,799",don't know decision tree ensembles of
462,"00:19:39,280 --> 00:19:43,400",smaller size but very often performance
463,"00:19:41,799 --> 00:19:46,200",of the random force is better if the
464,"00:19:43,400 --> 00:19:48,480",Ensemble is large of course this also
465,"00:19:46,200 --> 00:19:52,039",increases computational costs but we can
466,"00:19:48,480 --> 00:19:54,559",fit trees quite efficiently nowadays
467,"00:19:52,039 --> 00:19:56,159",even on larger data so that is not super
468,"00:19:54,559 --> 00:19:59,039",restrictive if your data is not
469,"00:19:56,159 --> 00:20:00,440",extremely large but also if if you make
470,"00:19:59,039 --> 00:20:03,679",your Ensemble larger and larger and
471,"00:20:00,440 --> 00:20:05,440",larger yeah that might increase the
472,"00:20:03,679 --> 00:20:07,520",performance of The
473,"00:20:05,440 --> 00:20:09,039",Ensemble yeah more and more and more to
474,"00:20:07,520 --> 00:20:10,840",a certain degree but usually at some
475,"00:20:09,039 --> 00:20:13,559",point we Flatline yeah and they are only
476,"00:20:10,840 --> 00:20:16,679",diminishing returns for this increased
477,"00:20:13,559 --> 00:20:19,080",computational cost usually setting theal
478,"00:20:16,679 --> 00:20:21,240",size to something like 100 or to 500
479,"00:20:19,080 --> 00:20:23,559",that's usually a sensible default but we
480,"00:20:21,240 --> 00:20:26,520",can also inspect something like the
481,"00:20:23,559 --> 00:20:29,039",development of our production error when
482,"00:20:26,520 --> 00:20:31,440",we vary the number of trees and there
483,"00:20:29,039 --> 00:20:33,679",are some smarter techniques how to do
484,"00:20:31,440 --> 00:20:35,720",that instead of just running a naive
485,"00:20:33,679 --> 00:20:38,200",cross validation which is what I did
486,"00:20:35,720 --> 00:20:40,679",here because we haven't discussed the
487,"00:20:38,200 --> 00:20:43,440",so-called outof back error estimation of
488,"00:20:40,679 --> 00:20:45,080",the random Forest but we'll look at that
489,"00:20:43,440 --> 00:20:48,200",I'm pretty sure exactly in the next
490,"00:20:45,080 --> 00:20:51,039",session after this one here okay here's
491,"00:20:48,200 --> 00:20:53,640",some visualization of how our response
492,"00:20:51,039 --> 00:20:57,000",surface changes when we average over
493,"00:20:53,640 --> 00:20:59,039",more and more trees so here you can see
494,"00:20:57,000 --> 00:21:00,880",if what what happens on the the iris
495,"00:20:59,039 --> 00:21:05,159",data set if I just visualize that for
496,"00:21:00,880 --> 00:21:08,159",two two features and I'm just using one
497,"00:21:05,159 --> 00:21:10,480",single tree and because I'm expanding
498,"00:21:08,159 --> 00:21:12,799",the tree yeah nearly maximally you can
499,"00:21:10,480 --> 00:21:15,279",also see that the resulting model it
500,"00:21:12,799 --> 00:21:17,200",doesn't look fairly nice it looks I
501,"00:21:15,279 --> 00:21:19,919",would actually say quite overfitted and
502,"00:21:17,200 --> 00:21:22,000",yeah also somewhat unstable if you run
503,"00:21:19,919 --> 00:21:23,720",that again maybe with a different I
504,"00:21:22,000 --> 00:21:25,600",shouldn't say seed because the decision
505,"00:21:23,720 --> 00:21:27,120",tree algorithm is actually deterministic
506,"00:21:25,600 --> 00:21:29,520",but yeah maybe I would bootstrap the
507,"00:21:27,120 --> 00:21:31,360",data a little bit I would wiggle around
508,"00:21:29,520 --> 00:21:33,279",the data a little bit the tree might
509,"00:21:31,360 --> 00:21:36,400",look somewhat different and now what
510,"00:21:33,279 --> 00:21:38,840",happens if we use two sorry 10 trees
511,"00:21:36,400 --> 00:21:42,039",yeah we see the response surface becomes
512,"00:21:38,840 --> 00:21:44,559",smoother and here is 500 trees so that
513,"00:21:42,039 --> 00:21:46,799",would be the default in most of the
514,"00:21:44,559 --> 00:21:47,760",implementations I know in R you can see
515,"00:21:46,799 --> 00:21:48,960",something else which is quite
516,"00:21:47,760 --> 00:21:52,279",interesting right you can see the
517,"00:21:48,960 --> 00:21:54,159",decision boundaries between classes so
518,"00:21:52,279 --> 00:21:56,520",they have become I would say fairly
519,"00:21:54,159 --> 00:21:59,919",smooth fairly appropriate you can also
520,"00:21:56,520 --> 00:22:01,840",see how the underlying surface of the
521,"00:21:59,919 --> 00:22:03,799",probabilistic classifier looks like yes
522,"00:22:01,840 --> 00:22:06,600",I'm trying to show this again through
523,"00:22:03,799 --> 00:22:10,240",this Alpha blending here so colors
524,"00:22:06,600 --> 00:22:13,200",become yeah more more Blended towards
525,"00:22:10,240 --> 00:22:15,600",towards White if the probability
526,"00:22:13,200 --> 00:22:17,799",distribution becomes less crisp less
527,"00:22:15,600 --> 00:22:19,600",clear and it's not so clear which class
528,"00:22:17,799 --> 00:22:23,039",we would predict and especially if you
529,"00:22:19,600 --> 00:22:24,919",compare this here with the first single
530,"00:22:23,039 --> 00:22:26,919",decision tree you can see and you should
531,"00:22:24,919 --> 00:22:29,480",know this by now that decision trees can
532,"00:22:26,919 --> 00:22:32,320",only perform these axis
533,"00:22:29,480 --> 00:22:34,159",Peril splits we can do a bunch of these
534,"00:22:32,320 --> 00:22:36,640",but in order to get something like a
535,"00:22:34,159 --> 00:22:38,279",decision boundary which is more or less
536,"00:22:36,640 --> 00:22:40,600",something like a diagonal line here
537,"00:22:38,279 --> 00:22:43,120",between these two classes between and I
538,"00:22:40,600 --> 00:22:44,799",guess the green one is V colore that's
539,"00:22:43,120 --> 00:22:46,640",not so easy to produce with a single
540,"00:22:44,799 --> 00:22:48,000",tree with the random Forest we're
541,"00:22:46,640 --> 00:22:50,159",actually getting pretty close to that
542,"00:22:48,000 --> 00:22:52,000",because we're averaging out over so many
543,"00:22:50,159 --> 00:22:54,159",of these I don't know step functions
544,"00:22:52,000 --> 00:22:56,240",from the individual trees there has been
545,"00:22:54,159 --> 00:22:57,960",a certain discussion in the literature
546,"00:22:56,240 --> 00:22:59,960",whether random Forest can actually
547,"00:22:57,960 --> 00:23:03,600",overfit and sometimes that's I would say
548,"00:22:59,960 --> 00:23:06,480",a bit misquoted the discussion so so
549,"00:23:03,600 --> 00:23:08,520",bman also began discussing that and
550,"00:23:06,480 --> 00:23:10,279",thinking about that and sometimes this
551,"00:23:08,520 --> 00:23:13,559",is reported as random Force can't
552,"00:23:10,279 --> 00:23:16,760",overfit that really is wrong um so
553,"00:23:13,559 --> 00:23:20,000",Random Force really can overfit as can
554,"00:23:16,760 --> 00:23:22,559",any machine learning model so what we
555,"00:23:20,000 --> 00:23:25,080",actually mean sometimes by these by this
556,"00:23:22,559 --> 00:23:27,360",statement random Forest cannot overfit
557,"00:23:25,080 --> 00:23:30,159",which we probably should not phrase
558,"00:23:27,360 --> 00:23:32,039",exactly like that is that a they are
559,"00:23:30,159 --> 00:23:34,919",less prone to overfitting than
560,"00:23:32,039 --> 00:23:36,720",individual decision trees and that
561,"00:23:34,919 --> 00:23:39,200",that's because this randomization of the
562,"00:23:36,720 --> 00:23:41,080",trees and the resulting averaging helps
563,"00:23:39,200 --> 00:23:44,840",the other thing what we usually want to
564,"00:23:41,080 --> 00:23:47,880",say is that they can overfit but usually
565,"00:23:44,840 --> 00:23:51,039",what does not happen is if we run the
566,"00:23:47,880 --> 00:23:53,080",algorithm for a longer and longer time
567,"00:23:51,039 --> 00:23:56,039",and by that we would mean we increase or
568,"00:23:53,080 --> 00:23:59,880",Som size more and more and more than
569,"00:23:56,039 --> 00:24:02,679",this in uh this increase in in oral size
570,"00:23:59,880 --> 00:24:05,120",usually does not result in relatively
571,"00:24:02,679 --> 00:24:07,880",more yeah so it either makes our model
572,"00:24:05,120 --> 00:24:11,080",better or we Flatline but we will not
573,"00:24:07,880 --> 00:24:13,640",become worse this can actually sometimes
574,"00:24:11,080 --> 00:24:15,880",still happen it's quite unusual if you
575,"00:24:13,640 --> 00:24:19,200",want to look into the details there's a
576,"00:24:15,880 --> 00:24:22,240",paper here by yeah Philip HS and
577,"00:24:19,200 --> 00:24:24,960",anoles so colleagues of mine who looked
578,"00:24:22,240 --> 00:24:27,559",at this a bit more in detail and and
579,"00:24:24,960 --> 00:24:30,880",then published this in in jlr these are
580,"00:24:27,559 --> 00:24:32,960",I would say really usual scenarios or
581,"00:24:30,880 --> 00:24:35,080",not very common ones so usually what
582,"00:24:32,960 --> 00:24:37,760",happens is something like this here yeah
583,"00:24:35,080 --> 00:24:40,039",we're increasing the number of trees
584,"00:24:37,760 --> 00:24:42,240",yeah just doesn't help us a lot after
585,"00:24:40,039 --> 00:24:44,120",some point yeah so we Flatline we'll
586,"00:24:42,240 --> 00:24:46,880",still stand spend more computation time
587,"00:24:44,120 --> 00:24:48,679",for this and but we will not yeah will
588,"00:24:46,880 --> 00:24:50,760",will not really begin to over fet so
589,"00:24:48,679 --> 00:24:54,760",it's not really this usual U-shaped
590,"00:24:50,760 --> 00:24:57,360",curve we see when we and we might
591,"00:24:54,760 --> 00:24:59,760",increase complexity of our model the
592,"00:24:57,360 --> 00:25:01,080",thing is I guess it's debatable and
593,"00:24:59,760 --> 00:25:03,159",whether we really increasing the
594,"00:25:01,080 --> 00:25:04,679",complexity of a model by increasing the
595,"00:25:03,159 --> 00:25:07,039",number of trees because the we not
596,"00:25:04,679 --> 00:25:09,640",really running something like a boosting
597,"00:25:07,039 --> 00:25:11,960",algorithm here or some some iterative
598,"00:25:09,640 --> 00:25:14,559",optimization when we add in more trees
599,"00:25:11,960 --> 00:25:16,880",we're just adding in a further
600,"00:25:14,559 --> 00:25:18,559",randomized Ensemble member and then just
601,"00:25:16,880 --> 00:25:20,440",averaging out over things so we
602,"00:25:18,559 --> 00:25:23,720",optimizing with respect to what's
603,"00:25:20,440 --> 00:25:25,840",already there and aggressively reducing
604,"00:25:23,720 --> 00:25:27,919",our loss yeah also if you think about
605,"00:25:25,840 --> 00:25:30,039",the the algorithm of the random forest
606,"00:25:27,919 --> 00:25:32,480",and and lagging in general the loss
607,"00:25:30,039 --> 00:25:33,880",function is not really an input to that
608,"00:25:32,480 --> 00:25:35,720",now so that's an input to the base
609,"00:25:33,880 --> 00:25:39,240",learning algorithm we just do the
610,"00:25:35,720 --> 00:25:39,240",averaging and we just do the variance
611,"00:25:39,440 --> 00:25:44,880",reduction yeah guess final slides so
612,"00:25:42,480 --> 00:25:47,880",here's one further example we looked at
613,"00:25:44,880 --> 00:25:50,399",this before so we looked at bagging for
614,"00:25:47,880 --> 00:25:53,640",logistic regression what effect that has
615,"00:25:50,399 --> 00:25:55,559",and for K&N and for the card algorithm
616,"00:25:53,640 --> 00:25:59,000",and here is now the random Forest so
617,"00:25:55,559 --> 00:26:01,279",that's the bagging but
618,"00:25:59,000 --> 00:26:03,320",enhanced with all of this all of the
619,"00:26:01,279 --> 00:26:05,640",things I explained here in this in this
620,"00:26:03,320 --> 00:26:08,000",session so the simple randomization plus
621,"00:26:05,640 --> 00:26:11,399",the full expansion and that actually
622,"00:26:08,000 --> 00:26:13,399",works drastically better here on this
623,"00:26:11,399 --> 00:26:16,600",what is it ah it's again the spend data
624,"00:26:13,399 --> 00:26:20,240",set okay so here you can really see the
625,"00:26:16,600 --> 00:26:22,440",usual quite nice quite good performance
626,"00:26:20,240 --> 00:26:25,279",of the random forest in action yeah and
627,"00:26:22,440 --> 00:26:28,919",we took good care that the orals are all
628,"00:26:25,279 --> 00:26:30,799",of of the same size so it again is 100
629,"00:26:28,919 --> 00:26:33,720",base learners for all of the begging or
630,"00:26:30,799 --> 00:26:37,399",SS but also for the random
631,"00:26:33,720 --> 00:26:39,600",Forest short summary I'll do that very
632,"00:26:37,399 --> 00:26:42,039",briefly because we I guess we did that
633,"00:26:39,600 --> 00:26:44,520",quite well in the session so most
634,"00:26:42,039 --> 00:26:46,120",advantages of the trees they inherited
635,"00:26:44,520 --> 00:26:49,080",into the random Forest so we don't have
636,"00:26:46,120 --> 00:26:51,200",to do that much tree um pre-processing
637,"00:26:49,080 --> 00:26:52,880",because it's a tree based Ensemble the
638,"00:26:51,200 --> 00:26:55,200",algorithm is actually fairly easy to
639,"00:26:52,880 --> 00:26:57,520",paralyze we didn't discuss that but that
640,"00:26:55,200 --> 00:26:59,760",should be obvious it's a loop over
641,"00:26:57,520 --> 00:27:02,720",independent thing things the bagging
642,"00:26:59,760 --> 00:27:05,679",also so we can easily distribute this to
643,"00:27:02,720 --> 00:27:09,080",multiple multiple CPUs to multiple cores
644,"00:27:05,679 --> 00:27:12,279",it really often works well enough so if
645,"00:27:09,080 --> 00:27:14,919",you do a reasonable approach for a given
646,"00:27:12,279 --> 00:27:16,679",data set where you would maybe start
647,"00:27:14,919 --> 00:27:20,399",with fairly simple models so maybe
648,"00:27:16,679 --> 00:27:22,399",linear models maybe trees maybe Gams and
649,"00:27:20,399 --> 00:27:24,279",they don't work well enough and you're a
650,"00:27:22,399 --> 00:27:25,880",little bit unhappy about that because
651,"00:27:24,279 --> 00:27:27,840",you would like to actually use an
652,"00:27:25,880 --> 00:27:30,960",interpal model here on that task but the
653,"00:27:27,840 --> 00:27:32,520",model are not yeah not good enough and
654,"00:27:30,960 --> 00:27:35,240",you also do care a lot about productive
655,"00:27:32,520 --> 00:27:37,440",performance I guess my next try would
656,"00:27:35,240 --> 00:27:40,360",usually always be random Forest because
657,"00:27:37,440 --> 00:27:44,039",they very often perform fairly well and
658,"00:27:40,360 --> 00:27:47,519",you can often get away either without
659,"00:27:44,039 --> 00:27:49,720",hyper tuning at all or you can do fairly
660,"00:27:47,519 --> 00:27:52,039",simple not too expensive not too
661,"00:27:49,720 --> 00:27:55,360",complicated hyper tuning maybe just of M
662,"00:27:52,039 --> 00:27:56,440",dry May yeah maybe just of M try they
663,"00:27:55,360 --> 00:27:57,720",often work quite well on high
664,"00:27:56,440 --> 00:28:00,600",dimensional data so there are many
665,"00:27:57,720 --> 00:28:02,960",people biostats bioinformatics I don't
666,"00:28:00,600 --> 00:28:04,960",know with maybe I don't know Gene data
667,"00:28:02,960 --> 00:28:07,480",or something like that they like using
668,"00:28:04,960 --> 00:28:09,919",random force in these domains as well
669,"00:28:07,480 --> 00:28:12,960",and can also work quite well in
670,"00:28:09,919 --> 00:28:15,600",scenarios with higher noise they do have
671,"00:28:12,960 --> 00:28:17,960",some disadvantages so though they have
672,"00:28:15,600 --> 00:28:20,039",the same extrapolation problem as for
673,"00:28:17,960 --> 00:28:21,960",trees so trees I don't know in
674,"00:28:20,039 --> 00:28:24,799",regression it might look like that
675,"00:28:21,960 --> 00:28:26,679",actually not look like that I don't know
676,"00:28:24,799 --> 00:28:28,440",and that at some point we can't really
677,"00:28:26,679 --> 00:28:30,760",extrapolate very nicely from the this if
678,"00:28:28,440 --> 00:28:33,559",our data ends here right so just
679,"00:28:30,760 --> 00:28:35,840",continue this flat production Behavior
680,"00:28:33,559 --> 00:28:39,000",they are obviously harder to interpret
681,"00:28:35,840 --> 00:28:40,880",than trees so many extra tools are
682,"00:28:39,000 --> 00:28:43,120",nowadays available for interpreting
683,"00:28:40,880 --> 00:28:45,080",random Forest um so there are things
684,"00:28:43,120 --> 00:28:46,720",like feature importance techniques that
685,"00:28:45,080 --> 00:28:48,399",have been actually many of the things
686,"00:28:46,720 --> 00:28:50,200",many of these model agnostic techniques
687,"00:28:48,399 --> 00:28:52,519",that we nowadays like to look at in
688,"00:28:50,200 --> 00:28:54,240",interpal machine learning and in a I
689,"00:28:52,519 --> 00:28:56,360",very many qu quite a few of them have
690,"00:28:54,240 --> 00:28:58,240",actually been developed specifically in
691,"00:28:56,360 --> 00:29:01,080",the context of the renom forest or for
692,"00:28:58,240 --> 00:29:03,159",the random forest and then sometimes the
693,"00:29:01,080 --> 00:29:04,399",implementations can be somewhat memory
694,"00:29:03,159 --> 00:29:06,799",hungry because you're creating an
695,"00:29:04,399 --> 00:29:09,399",assemble over trees and you might be a
696,"00:29:06,799 --> 00:29:12,159",little bit unhappy about I don't know
697,"00:29:09,399 --> 00:29:13,720",prediction time because if you predict
698,"00:29:12,159 --> 00:29:16,080",an observation you have to push it
699,"00:29:13,720 --> 00:29:20,799",through that complete Ensemble
700,"00:29:16,080 --> 00:29:23,880",for not super large scale data sets in
701,"00:29:20,799 --> 00:29:25,799",in training or in production so if you
702,"00:29:23,880 --> 00:29:28,120",usually this is not an issue yeah so for
703,"00:29:25,799 --> 00:29:29,880",for many many many I would say fairly
704,"00:29:28,120 --> 00:29:31,240",realistic and normal data sets that
705,"00:29:29,880 --> 00:29:34,440",that's not an issue if you're creating
706,"00:29:31,240 --> 00:29:36,679","an assemble of I don't know 50,000 trees"
707,"00:29:34,440 --> 00:29:38,600",and you have to stream in prediction
708,"00:29:36,679 --> 00:29:40,880",over a super large
709,"00:29:38,600 --> 00:29:42,640",database maybe you want to worry of that
710,"00:29:40,880 --> 00:29:45,880",about that a bit but for many
711,"00:29:42,640 --> 00:29:47,399",applications this is not an issue okay I
712,"00:29:45,880 --> 00:29:49,360",hope this session was not too long and
713,"00:29:47,399 --> 00:29:51,159",you could still uh follow along nicely
714,"00:29:49,360 --> 00:29:53,080",this concludes our session on the random
715,"00:29:51,159 --> 00:29:55,440",Forest algorithm here see you in the
