hi Welcome to our next session here in our yeah chapter on the random Forest algorithm where in this session here I will now truly introduce and discuss the fundamentals of the random Forest algorithm okay we'll understand how this extends the idea of bagging we'll then spend some time to understand the general idea behind the decorrelation of trees so the feature subsampling in the ren Forest algorithm we'll discuss the effect of yeah um the I would say the most important hyper parames of the random Forest algorithm and then discuss whether random Forest can actually overfit answer little bit of foreshadowing yes they can Okay so let's start we have discussed the card algorithm before so classification and regression trees by bran that actually has quite a few appealing features it's quite interoperable so it's easy to understand it's easy to explain because it's a simple decision tree it is invariant under rank preserving Transformations so as long as we keep yeah the order of feature values fixed while we transform them so by shifting them scaling them or doing any other type of monotonous transformation the the resulting model the resulting decision tree will not be substantially influenced by this the location of the split points will vary but we're basically fitting the same type the same type of model model and particularly the a model with the same predictive power and performance trees can work with cacor categorical features as they can with numerical features and they even can work with data where there are missing values present in the features Yeah by using so-called surrogate splits now here's a quote from hasted tipani and Freedman which says trees have exactly only one aspect that prevents them from being an Ideal tool for predictive learning not sure whether we go that far but they say h they could be a bit more accurate yeah they're not the most powerful predictor the idea of the random Forest is to basically attack exactly that problem by creating a begging Ensemble on top of decision trees we'll see that I guess this will become somewhat obvious that we'll lose the inability aspect of the single tree but all of these other advantages of the card algorithm that inherited into the random forest and we will also create a usually a way stronger predictor by embling in comparison to using a single decision tree so how does this work now we create a begging Ensemble out of a bunch of decision trees we have discussed how that works right so I don't have to repeat this year again bman when he introduced that which I think was around 1998 eight he thought about this a little bit more more deeply and he wanted to figure out whether he can could even improve the predictive performance of such a begging Ensemble further and then he figured out that the correlation among the emble members might actually be a problem or that to State it more positively That by decorrelating The Ensemble members he could he could potentially improve productive performance and we'll discuss exactly that now here's some intuition behind that how does bagging how does bagging work we have our model fad and our model f hat is going to be a simple average across capital M base Learners we already know we at least have a little bit of intuition that in yeah in terms of prediction error the variance of such a model and that the variance here of our assemble because that's what we care about is made yeah yeah not the best thing so if the model is very unstable if it's very if it has a has a large variance that's usually also a that's a component in our generalization error in this case we actually like the variance of the underlying base learning algorithm because that's something we can now smooth out over and that we can attack and reduce but the variance of our resulting Ensemble that should be reasonably small if that's too high that's usually large component also in our generalization error now do we also have an intuition how variance goes down if we yeah look at some some fundamental theorems of probability Theory or statistics we can expect that variance goes down linearly if we average out over capital M random variables if they are independent okay if things are independent let's say they are IID distributed we compute their average and then we look at the variance of the average in comparison to the average of one single random variable variance goes down linearly now can we expect our Ensemble members here in our begging Ensemble to be independent likely wrong right can't really expect that because our individual Ensemble members me numbers will be correlated and this is because our bootstrap samples are um fairly similar right we are sampling from the same underlying um training data set the bootstrap datas are um yeah fairly similar and this will also make our resulting models somewhat similar and correlated now if that's the case how about then we do um yeah more uh deeper analysis of what the variance of our resulting model actually is okay let's let's do exactly that so just assume two things so this analysis here is a little bit simplified but I guess it's going to be instructive anyway so I'll assume that yeah and this is where I'm simplifying things a little bit so this here is of course a function these these base Learners kind of treat them here as a scalar random variable and I'm assuming they have all the same variance Sigma squ and I'm also assuming they have a fixed correlation between any yeah any two of them and that correlation all denote by row what is now the variance of our Ensemble fad we just insert our definition of the bagging Ensemble that's just an average yeah over these base Learners bad n now there is now a nice formula for what happens with the variance even if stuff is not completely independ depent if it would be independent I could now just draw the variance operator kind of in a in a linear fashion inside of the sum but here I can't do that because stuff is correlated but I can use this more general formula here we have this which in a certain sense looks a little bit like a binomial formula so these squar terms here but here I have these covariance terms between Pairs of random variables okay can write it like that and now the only thing I will do is I will insert my my constants my abbreviations so the variance of bad M so that's supposed to be Sigma squ so we'll insert that here and now there's a sum over M of these things so it's capital M of Sigma squ I can also insert my correlation here now I have to be a little bit careful because correlation is not the same as Co variance but correlation multiplied by the standard deviation of of the one variable multipli by the standard deviation of the other variable that's the same as Co variance and because they all have the same variance so it's co variance between I and J is or sorry M and J is simply row * Sigma * Sigma so * Sigma squ now all of these terms here in this sum they are now all also all the same it's a sum over let me check capital M over 2 PA so this is M * m - 1 / 2 we cop over the two oh and I forgot about this guy here so the one divided by m that's inside of the variant so we draw that to the outside I'm actually already doing that here this becomes 1 / m^2 and now you just simplify things a little bit you collect terms a bit I'll remove all of my annoying scribbling here and then we look at the resulting formula and that has actually pretty nice structure it's quite instructive so we can see here the sigma squ ided by m so that would be the optimal reduction in variance we could ever hope for okay if there would be no correlation and here the sigma squar that's the worst that can happen to us because that would be no reduction variance you could for example think of all models being exact copies of each other yeah so really deterministic copies then if you average those there will be no reduction variance right because you're just aing out averaging out over all of the same things now this formula here tells us that between this zero reduction in variance and between this optimal linear reduction in invariance what we get is a convex combination of the two so you can see the convex combination here by one minus row here and having the RO there and this convex combination is controlled by the correlation between test of models of random variables the stronger the correlation the less reduction you get the closer you are to this yeah to this zero reduction in variance and if so if for example yeah if if robot be one yeah then this here becomes zero and we just end up with this very bad scenario where there's no reduction and if R would be zero there would be no correlation yeah then we have in this scenario of shouldn't say independence because we don't know that whether things are normally distributed here but in this uncorrelated scenario where we get linear reduction this is somewhat simplified I'll add a caveat on the next slide but in general now the idea is can we maybe decorrelate trees bit more to reduce onal variance and that will maybe also help us to reduce production error I guess that is a valid path forward now and this is exactly what bman did so he now thought about a simple randomization procedure that he could bring into the tree fitting algorithm so what we are now doing is when we are fitting our tree we greedily construct more and more nodes in the tree we construct more and more splits and in each individual split and when we construct that we search for the best split and when we search for the best split we search over all potential features and then over all possible split points to find the one which would reduces our risk the most so this randomization procedure now says do not search over all features but only consider a certain subset and that random subset of the I don't know feasible features you're allowed to look at in Split finding that's randomly sampled again and again um per note you could do this also per tree but usually this is done on a per note basis if you look in some at some implementations especially in r this parameter is called M try I guess that might be for maximum number of features to try out for finding a split I'm not exactly sure whether that's the the interpretation of the m and we will only consider these features now for finding the best split so maybe here this is our complete let me actually switch my pen tool on again so that maybe that's our complete data set here so with our Target column here we might have a bunch of different features maybe there's four but now we will only allow color and length yeah for this I don't know um training data set of measurements of bananas or objects which can be bananas and maybe also something else and we will not only search over them okay um and the idea is I guess that's some pretty obvious so the more we decorrelate or we the smaller we set this mry here the more trees become decorrelated but also the more rendom our trees become in the extreme case for example for a specific node and you you might have a large number of features maybe 100 and M2 would be set to one so you now pick one random feature and then you only allowed to search for the best split point for that feature which might be a very bad one yeah it might not carry any information it might not even be associated with wi might be some something like a noise feature but so you but you're only allowed to use that to find a split at that note guess for a newe going to sample again but this makes the trees fairly random and this will also have a negative effect if you overdo that but hopefully there might be something like a sweet sweet spot in the Middle where this decorrelation helps to reduce variance as we discussed before but the trees are not becoming super random and this is usually exactly what happens I've created here to simp yeah visualizations to simple examples so I used the classification data set the spam data set I use this empty cars data set for regression which we've also seen before and then now varying mry as percentage of total features in my data set from 0 to 100 and I'm now plotting the cross validated error of these different mry settings and what you can see is that if we set mry to very low value to so maybe to one then this doesn't work too well at least in in this data set here so our trees are becoming way too random yeah we are not really finding uh good splits anymore in our trees and then the variance reduction also doesn't save us and then M becomes a bit larger and that works actually fairly well and at some point m is too large yeah and this would here for m equals 100% that would be the original begging algorithm but you can see that usually there's a spot where we perform actually considerably better than by just fixing mry to 100% And and basically running the original bagging algorithm so there is some Merit there is some benefit in running this this decorrelated version of The Ensemble quite good default values exist which have being empirically I would say nicely validated so for classification very often we set this m try here to square root of number of features so that's also due to Bron and for regression we usually set it to somewhat larger value so P ided by 3 usually works quite well I've even yeah looked at what would happen if I would have used these default settings here for my two examples turns out yeah I was quite lucky they do work quite well here especially for the classification case nearly perfect for regression I guess I could have used an even larger setting yeah the random Forest algorithm itself is not I would say super sensitive to its hyper parer settings and these defaults usually work fairly well so if you just stick to them if you don't do any tuning you usually I would say you are not completely wrong in being lazy here and not tuning your model but if you want to tune a parameter this um yeah this this feature sampling parameter this mry parameter is usually the most important one so you can tune that um but you usually get I would say less less improvements um than for for hyper parames of other Machining models but you can try it sometimes it does work and it's worthwhile um we can also control the depth of our trees or at the very least we can discuss whether we would use somewhat shallow or deeper trees here in random Forest so in random Forest we usually use fully expanded trees so we don't really do any aggressive early stopping of our tree expansion I think we also talked here in the lecture about po talk pruning of trees usually all of that is not really done so the tree is configured in a manner that it's either fully or let's say nearly fully expanded and that is also done in order to increase the variability um the variance of each individual tree to get more from this you from this smoothing in the begging procedure we do can control this a little bit or can discuss the effects a bit so for example there would be a parameter which controls the minimum number of observations in each decision tree node so that's usually called yeah Min node size and that if you are below below that Min node size you would not attempt a further split in your tree by setting that to a larger value you can actually restrict and constrain regularize the size of your trees in your emble the default for that is very often a low value sometimes around five can also vary a little bit between regression and classification usually for regression you set it to an even lower value I have again looked at what happen happens on the spam data set for classification and on the Mt cars data set for regression so especially for regression you can see a very clear effect here that now the smaller you set Min node size you could even set this to one the better this works this exactly like what I said for classification you set this to an even lower value and for even here on the spam data set it works better if you choose smaller values but sometimes can be okay to to constrain this a little bit there are also other parameters to restrict the depth for example the max depth parameter so you can just prescribe a value so I don't want trees which have more than 100 terminal nodes or which don't grow over more than eight levels or whatever usually we don't use that too often so in this Ranger package here which is a very efficient uh implementation of the random forest in C++ and which is connected to R in Python so that's usually not restricted at all and there are also alternative H perameters to control the depth of the tree you have seen them before and we studied decision trees specifically like minimal risk reduction to actually require from a split from the optimal split to really perform this size of termal nodes and so on here's a further important kind of special hyper parameter onor size so I discussed this already a little bit so usually random Forest perform better if the ooral size is quite large so there are sometimes people that for computational reasons mainly construct smaller ensembles I guess very often also if you do bagging on top of new networks but sometimes also people run I don't know decision tree ensembles of smaller size but very often performance of the random force is better if the Ensemble is large of course this also increases computational costs but we can fit trees quite efficiently nowadays even on larger data so that is not super restrictive if your data is not extremely large but also if if you make your Ensemble larger and larger and larger yeah that might increase the performance of The Ensemble yeah more and more and more to a certain degree but usually at some point we Flatline yeah and they are only diminishing returns for this increased computational cost usually setting theal size to something like 100 or to 500 that's usually a sensible default but we can also inspect something like the development of our production error when we vary the number of trees and there are some smarter techniques how to do that instead of just running a naive cross validation which is what I did here because we haven't discussed the so-called outof back error estimation of the random Forest but we'll look at that I'm pretty sure exactly in the next session after this one here okay here's some visualization of how our response surface changes when we average over more and more trees so here you can see if what what happens on the the iris data set if I just visualize that for two two features and I'm just using one single tree and because I'm expanding the tree yeah nearly maximally you can also see that the resulting model it doesn't look fairly nice it looks I would actually say quite overfitted and yeah also somewhat unstable if you run that again maybe with a different I shouldn't say seed because the decision tree algorithm is actually deterministic but yeah maybe I would bootstrap the data a little bit I would wiggle around the data a little bit the tree might look somewhat different and now what happens if we use two sorry 10 trees yeah we see the response surface becomes smoother and here is 500 trees so that would be the default in most of the implementations I know in R you can see something else which is quite interesting right you can see the decision boundaries between classes so they have become I would say fairly smooth fairly appropriate you can also see how the underlying surface of the probabilistic classifier looks like yes I'm trying to show this again through this Alpha blending here so colors become yeah more more Blended towards towards White if the probability distribution becomes less crisp less clear and it's not so clear which class we would predict and especially if you compare this here with the first single decision tree you can see and you should know this by now that decision trees can only perform these axis Peril splits we can do a bunch of these but in order to get something like a decision boundary which is more or less something like a diagonal line here between these two classes between and I guess the green one is V colore that's not so easy to produce with a single tree with the random Forest we're actually getting pretty close to that because we're averaging out over so many of these I don't know step functions from the individual trees there has been a certain discussion in the literature whether random Forest can actually overfit and sometimes that's I would say a bit misquoted the discussion so so bman also began discussing that and thinking about that and sometimes this is reported as random Force can't overfit that really is wrong um so Random Force really can overfit as can any machine learning model so what we actually mean sometimes by these by this statement random Forest cannot overfit which we probably should not phrase exactly like that is that a they are less prone to overfitting than individual decision trees and that that's because this randomization of the trees and the resulting averaging helps the other thing what we usually want to say is that they can overfit but usually what does not happen is if we run the algorithm for a longer and longer time and by that we would mean we increase or Som size more and more and more than this in uh this increase in in oral size usually does not result in relatively more yeah so it either makes our model better or we Flatline but we will not become worse this can actually sometimes still happen it's quite unusual if you want to look into the details there's a paper here by yeah Philip HS and anoles so colleagues of mine who looked at this a bit more in detail and and then published this in in jlr these are I would say really usual scenarios or not very common ones so usually what happens is something like this here yeah we're increasing the number of trees yeah just doesn't help us a lot after some point yeah so we Flatline we'll still stand spend more computation time for this and but we will not yeah will will not really begin to over fet so it's not really this usual U-shaped curve we see when we and we might increase complexity of our model the thing is I guess it's debatable and whether we really increasing the complexity of a model by increasing the number of trees because the we not really running something like a boosting algorithm here or some some iterative optimization when we add in more trees we're just adding in a further randomized Ensemble member and then just averaging out over things so we optimizing with respect to what's already there and aggressively reducing our loss yeah also if you think about the the algorithm of the random forest and and lagging in general the loss function is not really an input to that now so that's an input to the base learning algorithm we just do the averaging and we just do the variance reduction yeah guess final slides so here's one further example we looked at this before so we looked at bagging for logistic regression what effect that has and for K&N and for the card algorithm and here is now the random Forest so that's the bagging but enhanced with all of this all of the things I explained here in this in this session so the simple randomization plus the full expansion and that actually works drastically better here on this what is it ah it's again the spend data set okay so here you can really see the usual quite nice quite good performance of the random forest in action yeah and we took good care that the orals are all of of the same size so it again is 100 base learners for all of the begging or SS but also for the random Forest short summary I'll do that very briefly because we I guess we did that quite well in the session so most advantages of the trees they inherited into the random Forest so we don't have to do that much tree um pre-processing because it's a tree based Ensemble the algorithm is actually fairly easy to paralyze we didn't discuss that but that should be obvious it's a loop over independent thing things the bagging also so we can easily distribute this to multiple multiple CPUs to multiple cores it really often works well enough so if you do a reasonable approach for a given data set where you would maybe start with fairly simple models so maybe linear models maybe trees maybe Gams and they don't work well enough and you're a little bit unhappy about that because you would like to actually use an interpal model here on that task but the model are not yeah not good enough and you also do care a lot about productive performance I guess my next try would usually always be random Forest because they very often perform fairly well and you can often get away either without hyper tuning at all or you can do fairly simple not too expensive not too complicated hyper tuning maybe just of M dry May yeah maybe just of M try they often work quite well on high dimensional data so there are many people biostats bioinformatics I don't know with maybe I don't know Gene data or something like that they like using random force in these domains as well and can also work quite well in scenarios with higher noise they do have some disadvantages so though they have the same extrapolation problem as for trees so trees I don't know in regression it might look like that actually not look like that I don't know and that at some point we can't really extrapolate very nicely from the this if our data ends here right so just continue this flat production Behavior they are obviously harder to interpret than trees so many extra tools are nowadays available for interpreting random Forest um so there are things like feature importance techniques that have been actually many of the things many of these model agnostic techniques that we nowadays like to look at in interpal machine learning and in a I very many qu quite a few of them have actually been developed specifically in the context of the renom forest or for the random forest and then sometimes the implementations can be somewhat memory hungry because you're creating an assemble over trees and you might be a little bit unhappy about I don't know prediction time because if you predict an observation you have to push it through that complete Ensemble for not super large scale data sets in in training or in production so if you usually this is not an issue yeah so for for many many many I would say fairly realistic and normal data sets that that's not an issue if you're creating an assemble of I don't know 50,000 trees and you have to stream in prediction over a super large database maybe you want to worry of that about that a bit but for many applications this is not an issue okay I hope this session was not too long and you could still uh follow along nicely this concludes our session on the random Forest algorithm here see you in the next one