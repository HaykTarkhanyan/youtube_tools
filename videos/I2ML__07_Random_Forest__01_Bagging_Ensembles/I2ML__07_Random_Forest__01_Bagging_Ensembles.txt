hi and Welcome to our new chapter here on random Forest so I will begin in in this session to introduce what the begging algorithm actually does so begging is for bootstrap aggregation and it's a preliminary version of the random Forest but can be applied to any type of machine learning model so we'll discuss the basic idea of begging here in the session then explain yeah that it's based on wood strabbing I guess that's already in the name and then we'll also yeah I'm going try to understand a bit more why this actually improves predictive performance especially if we apply bagging on top of somewhat random unstable learning algorithms okay so already explained the name so the B and bagging is for bootstrapping and the yeah egging is for for aggregation so really easily easy to explain how this works so the The General idea is to create an ensemble method it's the simplest Ensemble met method out there in machine learning so idea is to create multiple ml models fit them on somewhat similar data generated from the training data and yeah to Yeah by that idea to hopefully improve their productive performance somehow okay and how that specifically works I'll explain here for bagging and we'll see multiple other instances of assemble methods like boosting like stacking and so on in other parts of the lecture so yeah the individual machine learning models that are that The Ensemble is Created from they're usually called base Learners and there are so-called homogeneous ensembles that always use the same class the same type of Bas Learners so for example a cart a cart tree and that's exactly what we're going to use later on when we construct the random Forest but they also heterogeneous orable methods that can actually combine different types of BAS Learners which will then also usually have various degrees of protective performance and that we usually have to handle specifically begging is a homogeneous method so we'll always create similar models in our bagging algorithm because we'll always apply the same type of Base learning algorithm on the other hand it is a so-called model agnostic technique so we can use bagging together with any type of model at least technically we'll discuss at the end of this session where it actually makes the most sense and where it's maybe not so useful so how does this work as an algorithm it's actually fairly simple you can basically explain this on one slide I guess I'm trying to do that here I guess that'ss the training algorithm here on one slide for bagging so what we do is we take our original training data set D which you can also see here and we f fix the size of the Ensemble so our Ensemble our resulting Ensemble should be of size capital M that's how many models we want to want to generate in the end so what we will now do is that we will generate m capital M bootstrap samples of our original training data set D to generate some variation in our data so we take our original data set and then we simply will draw M bootstrap variations of that original data set which means we will draw with replacement from the original data set so draw an observation draw another observation draw another observation but with replacement until we have drawn we have sampled n of these so until we have sampled as many as we had before in the original data set and then on each of these bootstrapped data sets I'll simply fit my base learning algorithm and create a model okay and then I'll store these models that's that's really it there's not much more to say about the training algorithm if you haven't seen boot straming before yeah let's maybe discuss the the sampling with replacement a little bit well if we sample with replacement what's going to happen there is a certain possibility that some of the observations from the original training data set will be sampled multiple times right so I guess you should be able to see this here through the colors so this guy is sampled twice here in this data set and I guess this here is also sampled twice but can happen multiple times right all of the points that we have sampled for a specific set in or in a specific bootstrap iteration we would call them in bag yeah so that in the bag we sample them and because we can sample points multiple times there will be also certain points we will not sample and these points are called out of B so maybe I guess we can also check that visually so if we for example look at this thing here what have we not sampled I guess this here would be out of back for yeah the first bootstrap okay that's just a little bit of terminology now how do we predict with an emble it's even simpler or how how do we predict with a bagging Ensemble I should say well we take our new observation ax we push it into each of the Ensemble members we create a prediction yeah and here I'm looking at regression to start with the simplest case then I get capital M predictions and just average them and that's it here's the algorithm as pseudo code I will really not read that out now because it is so simple uh we' have already discussed that there's nothing new in the training in the training phase and the only thing I did here for the prodection phase which is I just made this more deta detailed and more explicit for what happens in classification which is always a little bit more annoying because you have these different uh types of outputs so you could have trained hard labeling based Learners which output discrete classes you could have trained something that predicts decision scores or you could have trained something that outputs predicts probabilities and from these different variations you might now construct the same types of outputs for your resulting ensemble so there's a bunch of different ways of aggregating things now so if you for example have decision scores well that's basically like in a regression model so you just average them as before if you have produced hard labels in your Bas Learners You Can Count yeah Over The Ensemble and by yeah and then you can do majority voting so you would predict the class that is predicted by most of the Ensemble members but you could also count again over these heart labels and then count proportions and you might treat that as a probability estimate for your assael if you want to produce probabilities or maybe you have trained um probabilistic classifiers and then can average their values to get a resulting probabilistic classifier for your orble so I'll not go into more details here most of these things work very similarly fairly easy to understand and in terms of a performance differences um um they are not large okay so it's it's bit unclear which of these is better why or when does begging help so what's the reason that this might be a good technique so the whole idea behind this and we'll explore this a little bit later in more detail when I uh introduce the complete random Forest so the idea behind bagging is that it should reduce the variability of our produtions because we average over them we smooth out right when we create these ensembles and then we average in production now that's a smoothing procedure it's the oldest trick in the book of Statistics if anything is like of two high variant yeah we run it multiple times and we average so this is particularly effective yeah if the if large parts of the prediction error of our underlying base Learners is usually due to that instability and often due and due to some some Randomness some random noise in that in that learning procedure so what you can see here is I've yeah created a bunch of different decision trees very often combine begging with decision trees that because decision trees are somewhat unstable um as a learning algorithm we have you we have learned that before and I'm now creating here yeah different bagging ensembles based on trees for different sizes right and here on the y- AIS you can see the resulting MSE on the regression task that I'm applying that to and you can see how that goes down when Ensemble size goes up on this and on this plot you can see the underlying data distribution as you can also see the the true function that I've sampled from so again something like a sinusoidal curve and here you can see the predictions of the individual trees yeah as these greenish yeah greenish confidence areas in the background you can also see the mean of the backed ensembles and so usually this reduction of variance is a good thing also in terms smooth it Smooths out the response surface it usually results in a reduction in prediction error and the larger The Ensemble the better this usually works up to a point and this the optimal Ensemble size usually depends on the specific learning algorithm you're using and also obviously depends on the data distribution here's a small Benchmark so we backed our sbls with about 100 base Learners here I used the spam data set which we've also seen before and I'm applying that on top of logistic regression which is a somewhat stable learning algorithm so there's not too much yeah instability random variation in that algorithm there is the card algorithm yeah that is somewhat unstable you can see that here using bagging on top of that improves the result by quite a large margin although the specification of the card algorithm here doesn't seem to work yeah too well in comparison to our logistic regression model and also in comparison here to a k nearest neighbor algorithm and that's also that depends a little bit on the setting of the case so you use the medium setting here for for k for the number of nearest neighbors but yeah both both of these so the logistic regression and the K's neighbor they are somewhat improved by bagging but not too much which is because they are a bit more stable yeah so for specifically true for the logistic regression but for the K andn depends a bit on the on the setting of the K so for smaller case it will become a bit more unstable and for a larger case it will be a bit more stable is also quite often applied to new networks often fitted through randomized optimization procedure procedures like a stochastic radi descent and if you then fit your n network on a smaller data set which might happen yeah not every data set is large scale and for for some reasons you might still prefer to run a new network procedure and then very often some some form of bagging is applied there to stabilize yeah the resulting Ensemble sometimes they only do this this even over different seats yeah they might not even bootstrap the data set okay this concludes our introductory session here I I'll stop now and I hope to see you again in the next session where I will then really introduce the random Forest bye-bye