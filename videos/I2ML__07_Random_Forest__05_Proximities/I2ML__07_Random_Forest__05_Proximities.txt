welcome back to introduction to machine learning I'm lud bman and in this video I will tell you about proximities and random forests first we have to understand how random forests can be used to define proximities and then we want to learn about how these proximities can be used and for which uh use cases these can be used so first about Computing these proximities so the goal is to compute proximities between observations okay so you have an observation x i and an observation XJ and you want to know how far they are away from each other or how near they are to each other so here we look at two IDs one and two and at the features of these observations so we are not looking at the Target variables but just at the features so in this case color form and length and what we do here is is that first we train uh random forest and after training we push all observations through each tree so we put or push observation one through tree 1 tre2 and tree3 and the same for observation two goes through all three trees and then we just compare the predictions of each of all these trees and we count how many times a tree predicts the same class for those two observations so in this case tree one predicts different Target variables for observation one and two but trees two and three predict the same value okay so in two or three cases they have the same value so proximity of these two observations is 2 / 3 now we can do this for all the observations in the data set so we have n observations in a data set so we get n * n or a matrix with n * n entries which is of course a symmetric Matrix because X1 and X2 have the same proximity as X2 and X1 sure what can we do with these proximities first we can try to just visualize the data set that we have so usually a data set has more than just two variables so it's hard to have a good plot of this data set because you well have too many dimensions to fit on a screen or on the paper so what we can do is we compute this proximity Matrix so each row and each column of this proximity Matrix refer to one observation of course one observation is always perfectly to to itself but others are perhaps not and then we can use this proximity Matrix perhaps first we have to take the inverse to compute distances but then we can visualize these distances for example what we see here is that well the the three classes of this penguin data set ardelia chinstrap and gen two can be seen here from this plot but what we Al also see is that for example this class Adelia has a r the high inclass variance so values are in this in this area as opposed to gen two which are perhaps not that diverse inside the class okay so we have perhaps learned something about the data set and we also see that well these two classes and also the third class overlap with each other which perhaps will make it challenging to train a good classification model right also using this visualization we can locate outliers for example this point over here belongs to the class chinstrap but well it's quite far away from all the other observations so yeah it's a perhaps untypical data point of this class we could also use this to have a look for mislabeled data points this can also happen especially in manually labeled data sets or data set where the target variable is defined by a human inspector somehow so we have to I guess a lot in for example medical contexts or other computer vision tasks where people are asked to label images so perhaps they also commit errors and perhaps we can see um some of these mislabeled data points with these proximity visualizations as well as third use case of these proximities is the imputation of missing data points so let's say we have this data set here with four columns color form original length or four features and we have missing variables this one and that one and or missing yeah missing observations here and what we could do of course is to replace all these missings with the median of each column okay so that's totally fine we can compute that here we did that so we just imputed those two observations with the median of these two observations which is brown and 14 in these two cases and now that we have a data set without missing values we can compute the proximities okay so we have to do some imputation first to compute the missing values but yeah now we can do this that's the second step now we've computed the proximities after a data set has changed and now we can replace the missings by a weighted average of the non-m missings so let's say we look at the column length here and remember that this observation was missing beforehand so after imputing the median of all the missing values we can compute the proximities and for example the third observation and the second observations seem to be not that far away so we take a weighted average of 14 19 and and 14 but 19 gets a high weight because this proximity between these two observations is rather High okay so we could say we are done but because we computed the proximities on the rather under complex imputation by just imputing the median perhaps we can do this again and again to get rid of this under complex imputation and have this kind of better imputation with the weighted average using proximities in a random Forest