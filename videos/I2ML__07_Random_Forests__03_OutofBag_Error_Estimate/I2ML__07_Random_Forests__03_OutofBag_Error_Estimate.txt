welcome back to introduction to machine learning I'm ludrick botman and in this video I will speak about outof b error estimate first you should understand the concept of outof back observations and inback observations and then you will learn how to use these outof back observations to get a proper estimate of the generalization error during training but first we have to get clear what out of bag and in bag observations are here you see first the visualization and then a more formal definition of what happens here so remember that in begging we use bootstrap to draw several data sets from the original entire data set uh with replacement so let's say we start with with this data set here it has three observations one two and three the column banana is the target variable the others are features and in the first bootstrap iteration we sample three observations from these three observations but with replacement so in this case it happens that the second sorry that the second observation does not make it into the bootstrap sample so we have the first observation and we duplicate the third observation and so we just we just have observations one and three in the bootstrap sample but three two times so so and then later we use this data set to train a model could be a tree for for random forests it will be a tree of course and then later we can use the observation that we did not use for training the tree so the second observation to predict its Target variable with this trained tree so we have inback observations these are exactly the observations that are inside the M bootstrap sample so let's have an example here perhaps you stop the video and try to figure it out yourself before I write it down okay so I B of one is or are all the indices indices of all observations that are in the bootstrap samples so one and three so it's exactly this set okay and the opposite holds for the out ofb observations these are exactly the observations that are not inside the M bootstrap sample so o o b of one will be exactly two so just this second line here and then we can do this for all the trees here we only see it for one tree but when we do it for all the trees then we can in the end also compute the number of trees where one observation is out of back so we can't have the example here because we just see one bootstrap sample but then we can use compute for each observation in how many trees it is out of back okay so what do we do with these out of back observation now we predict for all these or predict for the E observation with all the trees for which it is out of back so on the last slide we just had one bootstrap sample and just one trained tree okay here we have now four bootstrap samples which you don't see but you see the four trees that were trained on these four bootstrap samples we still have those three observations 1 2 and three um and now we have a further column the last one here this tells us in which of the trees this observation is out of back so we already know that for the first observation sorry for the second observation this was out of back in the first tree okay that's the same tree as before here so we see here it's out of back for tree one and and it turns out that it's not only out of back for tree one but it's also out of back for trees three and four so what we do now to compute the out of back prediction for observation two okay so we take observation two I want to compute the out of back prediction okay let that s in and then we'll see how that works so we used the second observation and predicted with all the trees where it's out of back so that's one three one three and four so we don't look at three two that's not of Interest right now but we predict observation 2 with 31 that gives us banana equals yes as a prediction with tree three which also predicts yes and Tre four which predicts no so since we know the true label here because we're on the training data we know that the true labels we can compare how often the prediction was correct and how often it was incorrect and in this case two of those predictions namely from 3 1 and tree3 were correct and one predict namely from 34 is incorrect okay so in summation in summation the out ofb prediction will be 2 ided by 3 because we have yeah in two two cases we were correct so we can evaluate okay so perhaps to to to say this once again the this is the outof back prediction which is 2 / 3 because in two cases we were in the class one so this assumes so this auto back prediction assumes that yes equals 1 and no no equal zero okay so this does not tell us if we are correct or not but here we see if we are correct or not okay so we can now do this for all the observations also for observation one and three also comput comput the outof back prediction here and then we can use all those a ofb predictions and use some loss function or set based evaluation metric to estimate the generalization error and the good thing is that this estimated generalization error is not optimistically biased because we do not violate violate the untouched test set principle here you have the pseudo code for exactly the same thing that I've explained before so what we need as an input are all those sets where we see which observation is out of back one of course all the Ensemble members that are trained and of course all the training data and then for each observation we go through Loop and compute the o o prediction for this observation for regression we usually use f fad but for classification you could also use pad also fad or perhaps the hard label classifier doesn't really matter so what you do is you go through all the trees and if the observation is out of back in that tree then this thing will be one if it's not out of beon at zero so it doesn't really matter what what comes behind this but if the observation start of back in that tree then you just look at the prediction from that tree for this observation features so F had M of XI and then in the end of course you have to normalize this so that this gets to be an average and then you have the out ofb prediction of the for the E observation and then you can use all these out of back predictions compare them for example with some loss function but again you can also use a set based evaluation matric row here and just go over all these observations here average the losses and you get not optimistically biased estimation of the generalization error yeah what can you do with this so what you see here is a small visualization for the spam data set so classific ation binary Target is it a spam email or not on the xaxis you see the number of trees of the random forest and on the Y AIS the misclassification error and the three lines correspond to the out of back error estimates and the middle line here is for the entire data set and the other two lines are just for class zero and just for class one so just Spam that one or just nons Spam this one um yeah and good thing is we get a proper estimator of GE and we can compute it during training and the nice thing is that if we have trained an ensemble with size m a capital M then we can directly look at all smaller number of trees as well right so we can just look at the first M half trees or some something something like that and have a first impression of how this hyper parameter affects the generalization error of the entire emble so one question could be how large this a of back set usually is and it turns out that the probability that an observation is inside the or is out of back for a given bootstrap sample converges to 1 / by E so more or less 1/3 so it's similar to hold out with two to one uh split or to fre full cross relation of course it's not the same thing yeah sure but it it feels a little bit like this because it's also more or less 1/3 for validation data so some final remarks here the O error is really unique to random forests or begging so you can't use this out of the box for some other machine learning algorithms or Learners so if you want to compare different models or different Learners with different type of parameter combination like also compare kous neighbors or a neural network or something like that then you should rather use cross validation hold out subsuming bootstrap you name it just to be consistent because we don't have OB error for these other Learners right but you can use the OB error to get a first impression of the random Forest performance if you just say okay I have a fixed hyper parameter set I'm only interested in the generalization error of this parameter combination then you can definitely use the OB error you can also use it to select your aoral size so you just train a large aoral and then look at the smaller oror numbers as well to see which ooral size is optimal and you can rather efficiently evaluate different random Forest hyperparameter configurations for this last point of course if you now look at a different High parameter than N3 so max step for example or mry then of course you have to train these ensembles also with those other hyper parameter combinations so of course if you want to see how different M Tri Par hyper parameter configurations compare to each other you have to train those models before you can compare the generalization errors there