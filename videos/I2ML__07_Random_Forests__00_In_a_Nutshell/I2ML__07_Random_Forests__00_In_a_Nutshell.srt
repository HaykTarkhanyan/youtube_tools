1
00:00:00,240 --> 00:00:05,000
hello and welcome back to introduction

2
00:00:02,360 --> 00:00:06,640
to machine learning I'm ludik bman and

3
00:00:05,000 --> 00:00:10,240
in this video I will tell you about

4
00:00:06,640 --> 00:00:12,440
random forests in a nutshell in the end

5
00:00:10,240 --> 00:00:14,799
you should understand the basic concept

6
00:00:12,440 --> 00:00:18,000
of random Forest you should know some

7
00:00:14,799 --> 00:00:22,560
basic aggregation rules and understand

8
00:00:18,000 --> 00:00:22,560
at least a basic concept of feature

9
00:00:23,439 --> 00:00:30,560
importance what is a random Forest you

10
00:00:27,599 --> 00:00:33,559
already know tree learners and with

11
00:00:30,560 --> 00:00:36,120
random forests we try to stabilize those

12
00:00:33,559 --> 00:00:38,320
tree Learners by begging begging is

13
00:00:36,120 --> 00:00:41,200
short for boot strap

14
00:00:38,320 --> 00:00:43,039
agregation and I will explain you what

15
00:00:41,200 --> 00:00:47,800
bootstrap aggregation

16
00:00:43,039 --> 00:00:51,079
means so what happens here is we have

17
00:00:47,800 --> 00:00:53,360
again labeled training data set with

18
00:00:51,079 --> 00:00:53,360
three

19
00:00:54,160 --> 00:01:01,160
observations three features and one

20
00:00:58,280 --> 00:01:05,360
categorical Target Vari

21
00:01:01,160 --> 00:01:09,280
variable so what bootstrap means is that

22
00:01:05,360 --> 00:01:11,840
we generate new training data sets out

23
00:01:09,280 --> 00:01:15,000
of the one training data set that we

24
00:01:11,840 --> 00:01:17,720
have yeah so we can't we cannot

25
00:01:15,000 --> 00:01:20,479
magically come up with new observations

26
00:01:17,720 --> 00:01:24,320
so what we do is we just draw with

27
00:01:20,479 --> 00:01:27,040
replacement from the training data so if

28
00:01:24,320 --> 00:01:29,960
you look at the first bootstrap sample

29
00:01:27,040 --> 00:01:33,079
that it has also three observations such

30
00:01:29,960 --> 00:01:35,479
as the original training data set but

31
00:01:33,079 --> 00:01:38,960
not all of these observations are

32
00:01:35,479 --> 00:01:41,159
different okay the observation with age

33
00:01:38,960 --> 00:01:44,680
23 comes up

34
00:01:41,159 --> 00:01:49,439
twice and the observation with age

35
00:01:44,680 --> 00:01:52,479
70 is missing again the observation with

36
00:01:49,439 --> 00:01:55,759
age 48 is there once and we do this

37
00:01:52,479 --> 00:01:57,759
several times let's say m times and in

38
00:01:55,759 --> 00:02:00,520
other bootstrap samples other

39
00:01:57,759 --> 00:02:04,479
observations may be apparent

40
00:02:00,520 --> 00:02:06,000
more than on such as the 70 H equal 70

41
00:02:04,479 --> 00:02:08,959
observations here in the last bootstrap

42
00:02:06,000 --> 00:02:11,480
sample and others may not show up at all

43
00:02:08,959 --> 00:02:16,000
in these in this bootstrap

44
00:02:11,480 --> 00:02:21,160
sample what we do then is we take these

45
00:02:16,000 --> 00:02:24,000
new data sets and for each data set we

46
00:02:21,160 --> 00:02:26,400
learn a tree separately and

47
00:02:24,000 --> 00:02:29,840
independently of the other trees so this

48
00:02:26,400 --> 00:02:32,200
means we get a lot of trees well that

49
00:02:29,840 --> 00:02:34,840
that's why we call it forest in the end

50
00:02:32,200 --> 00:02:36,519
so we have a lot of trees and all of

51
00:02:34,840 --> 00:02:40,040
these trees are well learned

52
00:02:36,519 --> 00:02:42,879
independently from each other so when we

53
00:02:40,040 --> 00:02:45,400
have a new observation coming in for

54
00:02:42,879 --> 00:02:47,519
prediction of course we have to decide

55
00:02:45,400 --> 00:02:49,840
well what do we do with it so we pass it

56
00:02:47,519 --> 00:02:52,800
through all the trees and then in the

57
00:02:49,840 --> 00:02:55,879
end we need some aggregation rule uh

58
00:02:52,800 --> 00:02:58,920
which tells us um how we um average the

59
00:02:55,879 --> 00:03:01,560
predictions of the different trees and

60
00:02:58,920 --> 00:03:03,040
that in the and is the entire random

61
00:03:01,560 --> 00:03:06,640
forest

62
00:03:03,040 --> 00:03:09,239
model random forests can be used for

63
00:03:06,640 --> 00:03:11,280
classification and regression because

64
00:03:09,239 --> 00:03:14,319
then as space loaders we just use a

65
00:03:11,280 --> 00:03:17,040
classification or regression

66
00:03:14,319 --> 00:03:18,920
trees and as already said at prediction

67
00:03:17,040 --> 00:03:20,799
time nothing really fancy happens

68
00:03:18,920 --> 00:03:24,239
because we have just learned random for

69
00:03:20,799 --> 00:03:26,599
a new supervis machine learning model so

70
00:03:24,239 --> 00:03:28,200
this picture here is the same as usually

71
00:03:26,599 --> 00:03:30,120
always just in the middle we have a

72
00:03:28,200 --> 00:03:31,760
random forest model instead of a tree

73
00:03:30,120 --> 00:03:34,159
learner or instead of a logistic

74
00:03:31,760 --> 00:03:36,400
regression or instead of whatever else

75
00:03:34,159 --> 00:03:38,720
okay so the fancy part is in the

76
00:03:36,400 --> 00:03:41,680
training part in the prediction it just

77
00:03:38,720 --> 00:03:45,000
looks and feels like any other machine

78
00:03:41,680 --> 00:03:45,000
super bied machine learning

79
00:03:46,560 --> 00:03:52,799
model okay I already said if we have

80
00:03:49,840 --> 00:03:55,720
several trees then each tree may give us

81
00:03:52,799 --> 00:03:58,400
a different prediction and now the task

82
00:03:55,720 --> 00:04:01,200
is to aggregate the predictions of the

83
00:03:58,400 --> 00:04:03,879
different tree learners

84
00:04:01,200 --> 00:04:07,319
so let's start here on the left hand

85
00:04:03,879 --> 00:04:12,000
side where we treat a classification

86
00:04:07,319 --> 00:04:14,720
task which means the target variable is

87
00:04:12,000 --> 00:04:17,280
a categorical variable and the in this

88
00:04:14,720 --> 00:04:20,359
case level of happiness again and we

89
00:04:17,280 --> 00:04:23,400
have one observation which we try to

90
00:04:20,359 --> 00:04:26,520
predict so we feed this prediction

91
00:04:23,400 --> 00:04:28,800
through all the trees we feed this

92
00:04:26,520 --> 00:04:32,160
observation through all the trees so we

93
00:04:28,800 --> 00:04:36,479
have five trees in the example and each

94
00:04:32,160 --> 00:04:36,479
tree gives one

95
00:04:36,720 --> 00:04:40,639
classification okay the first and the

96
00:04:38,560 --> 00:04:43,600
fifth say it's pretty happy and the

97
00:04:40,639 --> 00:04:46,520
other three say it's not too happy so

98
00:04:43,600 --> 00:04:49,039
what we can do is taking a majority vote

99
00:04:46,520 --> 00:04:51,240
of these so we have three trees voting

100
00:04:49,039 --> 00:04:53,120
for not too happy we have two trees

101
00:04:51,240 --> 00:04:55,800
voting for pretty happy and we have no

102
00:04:53,120 --> 00:04:59,759
tree predicting the third

103
00:04:55,800 --> 00:05:02,120
category so with taking the majority

104
00:04:59,759 --> 00:05:05,160
WOTE the final random Forest prediction

105
00:05:02,120 --> 00:05:07,960
is not too

106
00:05:05,160 --> 00:05:10,960
happy then on the right hand side we

107
00:05:07,960 --> 00:05:13,600
have a regression task so this means the

108
00:05:10,960 --> 00:05:18,199
target variable is a numerical

109
00:05:13,600 --> 00:05:20,720
variable and again we feed all the uh

110
00:05:18,199 --> 00:05:24,840
features through all the

111
00:05:20,720 --> 00:05:28,000
trees we get five predictions each tree

112
00:05:24,840 --> 00:05:31,319
has a unique or has a has its own

113
00:05:28,000 --> 00:05:33,360
prediction and we can do is for example

114
00:05:31,319 --> 00:05:36,240
just take the average so just the

115
00:05:33,360 --> 00:05:39,840
arithmetic mean of the predictions of

116
00:05:36,240 --> 00:05:43,080
the several trees and then we end up

117
00:05:39,840 --> 00:05:44,560
with one kind of forest prediction that

118
00:05:43,080 --> 00:05:47,199
example it's

119
00:05:44,560 --> 00:05:49,280
4842 okay whatever just some number but

120
00:05:47,199 --> 00:05:54,160
it's the empirical the arithmetic mean

121
00:05:49,280 --> 00:05:54,160
of those five individual tree

122
00:05:57,240 --> 00:06:03,600
predictions one note on the performance

123
00:06:00,039 --> 00:06:05,800
of random forests so well what's not on

124
00:06:03,600 --> 00:06:08,400
the slide but what's also very important

125
00:06:05,800 --> 00:06:11,319
is that random forests usually uh tend

126
00:06:08,400 --> 00:06:13,919
to perform really really well on tabular

127
00:06:11,319 --> 00:06:16,880
data sets it's hard to be random

128
00:06:13,919 --> 00:06:19,599
Forest um very much later you will

129
00:06:16,880 --> 00:06:21,240
perhaps see some uh boosted trees these

130
00:06:19,599 --> 00:06:25,000
perform very well as well but renom

131
00:06:21,240 --> 00:06:26,759
forests are quite well performing models

132
00:06:25,000 --> 00:06:28,000
so that's perhaps the most important

133
00:06:26,759 --> 00:06:30,800
part

134
00:06:28,000 --> 00:06:34,000
here in there's one rule increasing the

135
00:06:30,800 --> 00:06:36,280
assemble size stabilizes the predictions

136
00:06:34,000 --> 00:06:37,800
which you perhaps can see somewhere here

137
00:06:36,280 --> 00:06:39,800
so on the left hand side it's just one

138
00:06:37,800 --> 00:06:42,479
tree on the right hand side we have

139
00:06:39,800 --> 00:06:45,560
50,000 trees which is too much usually

140
00:06:42,479 --> 00:06:48,520
but this somehow this stabilizes the the

141
00:06:45,560 --> 00:06:50,360
prediction but still sometimes for

142
00:06:48,520 --> 00:06:52,960
regression task the stabilization is

143
00:06:50,360 --> 00:06:55,680
often not sufficient sufficient which

144
00:06:52,960 --> 00:06:57,879
means that we need some more tricks

145
00:06:55,680 --> 00:07:01,479
which will learn about of

146
00:06:57,879 --> 00:07:04,840
course but for classification tasks this

147
00:07:01,479 --> 00:07:08,199
works quite nicely so if we first just

148
00:07:04,840 --> 00:07:11,759
have a look at the second row

149
00:07:08,199 --> 00:07:15,360
here we have binary Target variable zero

150
00:07:11,759 --> 00:07:17,759
and one circles and triangles and you

151
00:07:15,360 --> 00:07:21,879
see the decision regions of two random

152
00:07:17,759 --> 00:07:24,520
forests these are a little bit different

153
00:07:21,879 --> 00:07:26,639
you can barely see it because they are

154
00:07:24,520 --> 00:07:29,639
so similar which is good so they are

155
00:07:26,639 --> 00:07:31,280
different because well the reinforce has

156
00:07:29,639 --> 00:07:33,840
two random components that's the

157
00:07:31,280 --> 00:07:36,280
bootstrapping and one thing which I tell

158
00:07:33,840 --> 00:07:38,960
you about on the next slide but still in

159
00:07:36,280 --> 00:07:41,319
the end they are pretty pretty similar

160
00:07:38,960 --> 00:07:43,879
okay which is a which is good and what

161
00:07:41,319 --> 00:07:46,919
in the first row here

162
00:07:43,879 --> 00:07:48,800
is the visualization of two different

163
00:07:46,919 --> 00:07:51,560
classification

164
00:07:48,800 --> 00:07:54,919
trees and what is that they are quite

165
00:07:51,560 --> 00:07:57,039
different okay if you look at this area

166
00:07:54,919 --> 00:07:59,400
here we have a totally different

167
00:07:57,039 --> 00:08:01,759
prediction and this can happen super

168
00:07:59,400 --> 00:08:04,319
super fast if just one observation

169
00:08:01,759 --> 00:08:06,440
changes the entire tree structure can

170
00:08:04,319 --> 00:08:09,680
change so they are very

171
00:08:06,440 --> 00:08:13,000
instable and random forests remedies

172
00:08:09,680 --> 00:08:13,000
this problem of tree

173
00:08:14,120 --> 00:08:19,960
Learners okay now the second trick I

174
00:08:17,039 --> 00:08:22,879
told you about on the slide before so

175
00:08:19,960 --> 00:08:26,080
what we try to do is we try to

176
00:08:22,879 --> 00:08:28,879
decorrelate the trees so we have

177
00:08:26,080 --> 00:08:31,879
different trees now so different Wes

178
00:08:28,879 --> 00:08:34,760
let's say and each of these trees makes

179
00:08:31,879 --> 00:08:37,680
mistakes of course but we want those

180
00:08:34,760 --> 00:08:40,240
trees to make different mistakes because

181
00:08:37,680 --> 00:08:42,200
if each tree makes a different mistake

182
00:08:40,240 --> 00:08:44,360
we can get rid of this problem by

183
00:08:42,200 --> 00:08:46,240
averaging out over the predictions of

184
00:08:44,360 --> 00:08:48,000
all the trees but if all the trees make

185
00:08:46,240 --> 00:08:52,360
the same mistake then averaging doesn't

186
00:08:48,000 --> 00:08:55,120
help at all okay so we avoid this Decor

187
00:08:52,360 --> 00:08:57,800
we avoid correlation by two things it's

188
00:08:55,120 --> 00:08:59,720
the bootstrap sampling so drawing with

189
00:08:57,800 --> 00:09:02,279
replacement from the training data and

190
00:08:59,720 --> 00:09:04,360
by randomizing splits okay what's

191
00:09:02,279 --> 00:09:09,200
randomizing

192
00:09:04,360 --> 00:09:11,480
splits you see here a note of a tree so

193
00:09:09,200 --> 00:09:14,320
here is the

194
00:09:11,480 --> 00:09:17,720
parent data feed here in and we have

195
00:09:14,320 --> 00:09:21,360
some child notes and what happens here

196
00:09:17,720 --> 00:09:23,079
is that in this note or let's start here

197
00:09:21,360 --> 00:09:26,120
we have six features available in the

198
00:09:23,079 --> 00:09:30,640
data set marital status health age sex

199
00:09:26,120 --> 00:09:33,720
and degree and when and searching for

200
00:09:30,640 --> 00:09:37,360
the best split in this Noe we do not

201
00:09:33,720 --> 00:09:40,320
search over all possible features but

202
00:09:37,360 --> 00:09:43,519
first we randomly draw some of those

203
00:09:40,320 --> 00:09:46,079
features so how many features we draw is

204
00:09:43,519 --> 00:09:49,839
the hyperparameter let's say we draw

205
00:09:46,079 --> 00:09:51,920
three features here so we have an random

206
00:09:49,839 --> 00:09:55,360
selection of the features and we just

207
00:09:51,920 --> 00:09:56,320
search for the best splits inside these

208
00:09:55,360 --> 00:09:58,720
three

209
00:09:56,320 --> 00:10:01,320
features and we do this for every note

210
00:09:58,720 --> 00:10:04,000
in every every tree so we take a new

211
00:10:01,320 --> 00:10:07,920
sample every in every note in every tree

212
00:10:04,000 --> 00:10:07,920
which also decorrelates the

213
00:10:08,399 --> 00:10:13,800
trees last but not least there is a nice

214
00:10:11,399 --> 00:10:15,519
thing called feature importance so the

215
00:10:13,800 --> 00:10:18,880
question we try to answer with feature

216
00:10:15,519 --> 00:10:21,000
importance is okay well which feature is

217
00:10:18,880 --> 00:10:23,959
most important for the prediction for

218
00:10:21,000 --> 00:10:25,880
the model of course so as you perhaps

219
00:10:23,959 --> 00:10:29,680
know from statistics with I don't know

220
00:10:25,880 --> 00:10:30,880
some P values or want know how how

221
00:10:29,680 --> 00:10:33,880
important is the

222
00:10:30,880 --> 00:10:35,880
feature and feature importance are the

223
00:10:33,880 --> 00:10:38,200
thing to go with random forest and they

224
00:10:35,880 --> 00:10:39,720
are very there are several options to do

225
00:10:38,200 --> 00:10:43,680
feature or to compute feature

226
00:10:39,720 --> 00:10:47,079
importances out of random forests

227
00:10:43,680 --> 00:10:49,680
one example is here that's the feature

228
00:10:47,079 --> 00:10:51,200
importance based on Improvement in the

229
00:10:49,680 --> 00:10:53,079
splitting criteria there are other and

230
00:10:51,200 --> 00:10:55,200
you will get to know those but that's

231
00:10:53,079 --> 00:10:59,760
just the first glimpse of feature

232
00:10:55,200 --> 00:11:03,240
importances here so how does this work

233
00:10:59,760 --> 00:11:07,800
we have still different trees so each

234
00:11:03,240 --> 00:11:12,000
row here is one tree we have in total M

235
00:11:07,800 --> 00:11:14,880
different trees and now in each tree if

236
00:11:12,000 --> 00:11:16,200
we want to know the feature importance

237
00:11:14,880 --> 00:11:19,880
of the feature

238
00:11:16,200 --> 00:11:24,240
Health we look in each

239
00:11:19,880 --> 00:11:27,040
tree for all the noes where we used

240
00:11:24,240 --> 00:11:30,800
Health as splitting

241
00:11:27,040 --> 00:11:34,600
feature Okay this may be zero one or

242
00:11:30,800 --> 00:11:36,680
several but it can be so yeah can be

243
00:11:34,600 --> 00:11:39,079
several Fe several notes where we used

244
00:11:36,680 --> 00:11:42,959
Health as splitting

245
00:11:39,079 --> 00:11:44,680
feature and then we compute how much

246
00:11:42,959 --> 00:11:47,040
this single

247
00:11:44,680 --> 00:11:48,920
split

248
00:11:47,040 --> 00:11:52,120
improved

249
00:11:48,920 --> 00:11:54,880
some quality measure which you will know

250
00:11:52,120 --> 00:11:57,560
about later but so the question is how

251
00:11:54,880 --> 00:11:59,600
much did this split and just only this

252
00:11:57,560 --> 00:12:03,360
very split in this very not

253
00:11:59,600 --> 00:12:06,680
improve some quality measure then we add

254
00:12:03,360 --> 00:12:08,440
up all those improvements for the tree

255
00:12:06,680 --> 00:12:11,200
then we have the improvement from One

256
00:12:08,440 --> 00:12:14,720
Tree and we do this for all the trees

257
00:12:11,200 --> 00:12:16,800
sum those up and then we have a final

258
00:12:14,720 --> 00:12:20,040
feature importance which tells us in

259
00:12:16,800 --> 00:12:25,839
this case how important the feature

260
00:12:20,040 --> 00:12:25,839
health is for the model predictions