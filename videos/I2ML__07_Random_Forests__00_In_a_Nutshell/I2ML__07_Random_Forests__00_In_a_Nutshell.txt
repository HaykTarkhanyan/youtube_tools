hello and welcome back to introduction to machine learning I'm ludik bman and in this video I will tell you about random forests in a nutshell in the end you should understand the basic concept of random Forest you should know some basic aggregation rules and understand at least a basic concept of feature importance what is a random Forest you already know tree learners and with random forests we try to stabilize those tree Learners by begging begging is short for boot strap agregation and I will explain you what bootstrap aggregation means so what happens here is we have again labeled training data set with three observations three features and one categorical Target Vari variable so what bootstrap means is that we generate new training data sets out of the one training data set that we have yeah so we can't we cannot magically come up with new observations so what we do is we just draw with replacement from the training data so if you look at the first bootstrap sample that it has also three observations such as the original training data set but not all of these observations are different okay the observation with age 23 comes up twice and the observation with age 70 is missing again the observation with age 48 is there once and we do this several times let's say m times and in other bootstrap samples other observations may be apparent more than on such as the 70 H equal 70 observations here in the last bootstrap sample and others may not show up at all in these in this bootstrap sample what we do then is we take these new data sets and for each data set we learn a tree separately and independently of the other trees so this means we get a lot of trees well that that's why we call it forest in the end so we have a lot of trees and all of these trees are well learned independently from each other so when we have a new observation coming in for prediction of course we have to decide well what do we do with it so we pass it through all the trees and then in the end we need some aggregation rule uh which tells us um how we um average the predictions of the different trees and that in the and is the entire random forest model random forests can be used for classification and regression because then as space loaders we just use a classification or regression trees and as already said at prediction time nothing really fancy happens because we have just learned random for a new supervis machine learning model so this picture here is the same as usually always just in the middle we have a random forest model instead of a tree learner or instead of a logistic regression or instead of whatever else okay so the fancy part is in the training part in the prediction it just looks and feels like any other machine super bied machine learning model okay I already said if we have several trees then each tree may give us a different prediction and now the task is to aggregate the predictions of the different tree learners so let's start here on the left hand side where we treat a classification task which means the target variable is a categorical variable and the in this case level of happiness again and we have one observation which we try to predict so we feed this prediction through all the trees we feed this observation through all the trees so we have five trees in the example and each tree gives one classification okay the first and the fifth say it's pretty happy and the other three say it's not too happy so what we can do is taking a majority vote of these so we have three trees voting for not too happy we have two trees voting for pretty happy and we have no tree predicting the third category so with taking the majority WOTE the final random Forest prediction is not too happy then on the right hand side we have a regression task so this means the target variable is a numerical variable and again we feed all the uh features through all the trees we get five predictions each tree has a unique or has a has its own prediction and we can do is for example just take the average so just the arithmetic mean of the predictions of the several trees and then we end up with one kind of forest prediction that example it's 4842 okay whatever just some number but it's the empirical the arithmetic mean of those five individual tree predictions one note on the performance of random forests so well what's not on the slide but what's also very important is that random forests usually uh tend to perform really really well on tabular data sets it's hard to be random Forest um very much later you will perhaps see some uh boosted trees these perform very well as well but renom forests are quite well performing models so that's perhaps the most important part here in there's one rule increasing the assemble size stabilizes the predictions which you perhaps can see somewhere here so on the left hand side it's just one tree on the right hand side we have 50,000 trees which is too much usually but this somehow this stabilizes the the prediction but still sometimes for regression task the stabilization is often not sufficient sufficient which means that we need some more tricks which will learn about of course but for classification tasks this works quite nicely so if we first just have a look at the second row here we have binary Target variable zero and one circles and triangles and you see the decision regions of two random forests these are a little bit different you can barely see it because they are so similar which is good so they are different because well the reinforce has two random components that's the bootstrapping and one thing which I tell you about on the next slide but still in the end they are pretty pretty similar okay which is a which is good and what in the first row here is the visualization of two different classification trees and what is that they are quite different okay if you look at this area here we have a totally different prediction and this can happen super super fast if just one observation changes the entire tree structure can change so they are very instable and random forests remedies this problem of tree Learners okay now the second trick I told you about on the slide before so what we try to do is we try to decorrelate the trees so we have different trees now so different Wes let's say and each of these trees makes mistakes of course but we want those trees to make different mistakes because if each tree makes a different mistake we can get rid of this problem by averaging out over the predictions of all the trees but if all the trees make the same mistake then averaging doesn't help at all okay so we avoid this Decor we avoid correlation by two things it's the bootstrap sampling so drawing with replacement from the training data and by randomizing splits okay what's randomizing splits you see here a note of a tree so here is the parent data feed here in and we have some child notes and what happens here is that in this note or let's start here we have six features available in the data set marital status health age sex and degree and when and searching for the best split in this Noe we do not search over all possible features but first we randomly draw some of those features so how many features we draw is the hyperparameter let's say we draw three features here so we have an random selection of the features and we just search for the best splits inside these three features and we do this for every note in every every tree so we take a new sample every in every note in every tree which also decorrelates the trees last but not least there is a nice thing called feature importance so the question we try to answer with feature importance is okay well which feature is most important for the prediction for the model of course so as you perhaps know from statistics with I don't know some P values or want know how how important is the feature and feature importance are the thing to go with random forest and they are very there are several options to do feature or to compute feature importances out of random forests one example is here that's the feature importance based on Improvement in the splitting criteria there are other and you will get to know those but that's just the first glimpse of feature importances here so how does this work we have still different trees so each row here is one tree we have in total M different trees and now in each tree if we want to know the feature importance of the feature Health we look in each tree for all the noes where we used Health as splitting feature Okay this may be zero one or several but it can be so yeah can be several Fe several notes where we used Health as splitting feature and then we compute how much this single split improved some quality measure which you will know about later but so the question is how much did this split and just only this very split in this very not improve some quality measure then we add up all those improvements for the tree then we have the improvement from One Tree and we do this for all the trees sum those up and then we have a final feature importance which tells us in this case how important the feature health is for the model predictions